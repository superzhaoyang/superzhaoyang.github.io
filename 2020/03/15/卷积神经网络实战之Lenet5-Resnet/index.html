<!DOCTYPE html><html lang="zh-Hans" data-theme="light"><head><meta name="generator" content="Hexo 3.9.0"><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><title>卷积神经网络实战之Lenet5 &amp; Resnet | superzhaoyang</title><meta name="description" content="卷积神经网络实战之Lenet5 &amp; Resnet"><meta name="keywords" content="deep learning,CNN,Lenet5,Resnet"><meta name="author" content="superzhaoyang"><meta name="copyright" content="superzhaoyang"><meta name="format-detection" content="telephone=no"><link rel="shortcut icon" href="/img/favicon.ico"><link rel="preconnect" href="//cdn.jsdelivr.net"><meta name="twitter:card" content="summary_large_image"><meta name="twitter:title" content="卷积神经网络实战之Lenet5 &amp; Resnet"><meta name="twitter:description" content="卷积神经网络实战之Lenet5 &amp; Resnet"><meta name="twitter:image" content="https://s1.ax1x.com/2020/03/15/88wucj.png"><meta property="og:type" content="article"><meta property="og:title" content="卷积神经网络实战之Lenet5 &amp; Resnet"><meta property="og:url" content="http://yoursite.com/2020/03/15/卷积神经网络实战之Lenet5-Resnet/"><meta property="og:site_name" content="superzhaoyang"><meta property="og:description" content="卷积神经网络实战之Lenet5 &amp; Resnet"><meta property="og:image" content="https://s1.ax1x.com/2020/03/15/88wucj.png"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><script src="https://cdn.jsdelivr.net/npm/js-cookie/dist/js.cookie.min.js"></script><script>const autoChangeMode = 'true'
var t = Cookies.get("theme");
if (autoChangeMode == '1'){
const isDarkMode = window.matchMedia("(prefers-color-scheme: dark)").matches
const isLightMode = window.matchMedia("(prefers-color-scheme: light)").matches
const isNotSpecified = window.matchMedia("(prefers-color-scheme: no-preference)").matches
const hasNoSupport = !isDarkMode && !isLightMode && !isNotSpecified

if (t === undefined){
  if (isLightMode) activateLightMode()
  else if (isDarkMode) activateDarkMode()
  else if (isNotSpecified || hasNoSupport){
    console.log('You specified no preference for a color scheme or your browser does not support it. I Schedule dark mode during night time.')
    now = new Date();
    hour = now.getHours();
    isNight = hour < 6 || hour >= 18
    isNight ? activateDarkMode() : activateLightMode()
}
} else if (t == 'light') activateLightMode()
else activateDarkMode()


} else if (autoChangeMode == '2'){
  now = new Date();
  hour = now.getHours();
  isNight = hour < 6 || hour >= 18
  if(t === undefined) isNight? activateDarkMode() : activateLightMode()
  else if (t === 'light') activateLightMode()
  else activateDarkMode() 
} else {
  if ( t == 'dark' ) activateDarkMode()
  else if ( t == 'light') activateLightMode()
}

function activateDarkMode(){
  document.documentElement.setAttribute('data-theme', 'dark')
  if (document.querySelector('meta[name="theme-color"]') !== null){
    document.querySelector('meta[name="theme-color"]').setAttribute('content','#000')
  }
}
function activateLightMode(){
  document.documentElement.setAttribute('data-theme', 'light')
  if (document.querySelector('meta[name="theme-color"]') !== null){
  document.querySelector('meta[name="theme-color"]').setAttribute('content','#fff')
  }
}</script><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="/fonts/font-awesome.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/node-snackbar@0.1.15/dist/snackbar.min.css"><link rel="canonical" href="http://yoursite.com/2020/03/15/卷积神经网络实战之Lenet5-Resnet/"><link rel="prev" title="关于Ubuntu 19.10安装nginx时遇到的问题" href="http://yoursite.com/2020/03/16/关于Ubuntu19.10下安装nginx遇到的问题/"><link rel="next" title="github实用搜索技巧" href="http://yoursite.com/2020/03/15/github实用搜索技巧/"><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Titillium+Web"><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"search.xml","languages":{"hits_empty":"We didn't find any results for the search: ${query}"}},
  translate: {"defaultEncoding":2,"translateDelay":0,"cookieDomain":"https://www.superzhaoyang.top/","msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"简"},
  highlight_copy: 'true',
  highlight_lang: 'true',
  highlight_shrink: 'false',
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  },
  bookmark: {
    title: 'Snackbar.bookmark.title',
    message_prev: 'Press',
    message_next: 'to bookmark this page'
  },
  runtime_unit: 'days',
  copyright: undefined,
  copy_copyright_js: false,
  ClickShowText: {"text":"superzhaoyang,帅,聪明,IT,敲代码,LOL,玩,酷,绚,NB,乒乓球,付同有我儿子","fontSize":"15px"},
  medium_zoom: 'true',
  Snackbar: {"bookmark":{"title":"Snackbar.bookmark.title","message_prev":"Press","message_next":"to bookmark this page"},"chs_to_cht":"Traditional Chinese Activated Manually","cht_to_chs":"Simplified Chinese Activated Manually","day_to_night":"Light Mode Activated Manually","night_to_day":"Dark Mode Activated Manually","bgLight":"#49b1f5","bgDark":"#2d3035","position":"bottom-left"}
  
}</script></head><body><canvas class="fireworks"></canvas><div id="header"> <div id="page-header"><span class="pull_left" id="blog_name"><a class="blog_title" id="site-name" href="/">superzhaoyang</a></span><i class="fa fa-bars fa-fw toggle-menu pull_right close" aria-hidden="true"></i><span class="pull_right menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fa fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fa fa-archive"></i><span> 档案</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fa fa-folder-open"></i><span> 类别</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fa fa-link"></i><span> 链接</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fa fa-heart"></i><span> 关于</span></a></div><div class="menus_item"><a class="site-page"><i class="fa-fw fa fa-list" aria-hidden="true"></i><span> 学习和娱乐</span><i class="fa fa-chevron-down menus-expand" aria-hidden="true"></i></a><ul class="menus_item_child"><li><a class="site-page" href="/music/"><i class="fa-fw fa fa-music"></i><span> 音乐</span></a></li><li><a class="site-page" href="/movies/"><i class="fa-fw fa fa-film"></i><span> 电影</span></a></li><li><a class="site-page" href="/book/"><i class="fa-fw fa fa-book"></i><span> 书籍</span></a></li></ul></div></div></span><span class="pull_right" id="search_button"><a class="site-page social-icon search"><i class="fa fa-search fa-fw"></i><span> Search</span></a></span></div></div><div id="mobile-sidebar"><div id="menu_mask"></div><div id="mobile-sidebar-menus"><div class="mobile_author_icon"><img class="lazyload avatar_img" src="https://s2.ax1x.com/2019/09/10/naEP1g.jpg" onerror="onerror=null;src='/img/friend_404.gif'"></div><div class="mobile_post_data"><div class="mobile_data_item is_center"><div class="mobile_data_link"><a href="/archives/"><div class="headline">Articles</div><div class="length_num">56</div></a></div></div><div class="mobile_data_item is_center">      <div class="mobile_data_link"><a href="/tags/"><div class="headline">Tags</div><div class="length_num">72</div></a></div></div></div><hr><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fa fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fa fa-archive"></i><span> 档案</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fa fa-folder-open"></i><span> 类别</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fa fa-link"></i><span> 链接</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fa fa-heart"></i><span> 关于</span></a></div><div class="menus_item"><a class="site-page"><i class="fa-fw fa fa-list" aria-hidden="true"></i><span> 学习和娱乐</span><i class="fa fa-chevron-down menus-expand" aria-hidden="true"></i></a><ul class="menus_item_child"><li><a class="site-page" href="/music/"><i class="fa-fw fa fa-music"></i><span> 音乐</span></a></li><li><a class="site-page" href="/movies/"><i class="fa-fw fa fa-film"></i><span> 电影</span></a></li><li><a class="site-page" href="/book/"><i class="fa-fw fa fa-book"></i><span> 书籍</span></a></li></ul></div></div></div><div id="mobile-sidebar-toc"><div class="toc_mobile_headline">Catalog</div><ol class="toc_mobile_items"><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#CNN最近几年的发展"><span class="toc_mobile_items-number">1.</span> <span class="toc_mobile_items-text">CNN最近几年的发展</span></a><ol class="toc_mobile_items-child"><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#AlexNet"><span class="toc_mobile_items-number">1.1.</span> <span class="toc_mobile_items-text">AlexNet</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#VGG"><span class="toc_mobile_items-number">1.2.</span> <span class="toc_mobile_items-text">VGG</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#GoogleNET"><span class="toc_mobile_items-number">1.3.</span> <span class="toc_mobile_items-text">GoogleNET</span></a></li></ol></li><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#LeNet5"><span class="toc_mobile_items-number">2.</span> <span class="toc_mobile_items-text">LeNet5</span></a><ol class="toc_mobile_items-child"><li class="toc_mobile_items-item toc_mobile_items-level-4"><a class="toc_mobile_items-link" href="#lenet5-py"><span class="toc_mobile_items-number">2.0.1.</span> <span class="toc_mobile_items-text">lenet5.py</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-4"><a class="toc_mobile_items-link" href="#main-py"><span class="toc_mobile_items-number">2.0.2.</span> <span class="toc_mobile_items-text">main.py</span></a></li></ol></li></ol><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#ResNet"><span class="toc_mobile_items-number">3.</span> <span class="toc_mobile_items-text">ResNet</span></a><ol class="toc_mobile_items-child"><li class="toc_mobile_items-item toc_mobile_items-level-4"><a class="toc_mobile_items-link" href="#ResNet-py"><span class="toc_mobile_items-number">3.0.1.</span> <span class="toc_mobile_items-text">ResNet.py</span></a></li></ol></li></div></div><div id="body-wrap"><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true">     </i><div class="auto_open" id="sidebar"><div class="sidebar-toc"><div class="sidebar-toc__title">Catalog</div><div class="sidebar-toc__progress"><span class="progress-notice">You've read</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar">     </div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#CNN最近几年的发展"><span class="toc-number">1.</span> <span class="toc-text">CNN最近几年的发展</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#AlexNet"><span class="toc-number">1.1.</span> <span class="toc-text">AlexNet</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#VGG"><span class="toc-number">1.2.</span> <span class="toc-text">VGG</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#GoogleNET"><span class="toc-number">1.3.</span> <span class="toc-text">GoogleNET</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#LeNet5"><span class="toc-number">2.</span> <span class="toc-text">LeNet5</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#lenet5-py"><span class="toc-number">2.0.1.</span> <span class="toc-text">lenet5.py</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#main-py"><span class="toc-number">2.0.2.</span> <span class="toc-text">main.py</span></a></li></ol></li></ol><li class="toc-item toc-level-2"><a class="toc-link" href="#ResNet"><span class="toc-number">3.</span> <span class="toc-text">ResNet</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#ResNet-py"><span class="toc-number">3.0.1.</span> <span class="toc-text">ResNet.py</span></a></li></ol></li></div></div></div><div id="content-outer"><div id="top-container" style="background-image: url(https://s1.ax1x.com/2020/03/15/88wucj.png)"><div id="post-info"><div id="post-title"><div class="posttitle">卷积神经网络实战之Lenet5 &amp; Resnet</div></div><div id="post-meta"><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> Created 2020-03-15<span class="post-meta__separator">|</span><i class="fa fa-history" aria-hidden="true"></i> Updated 2020-03-25</time><div class="post-meta-wordcount"><i class="fa fa-eye post-meta__icon" aria-hidden="true">       </i><span>Post View: </span><span id="busuanzi_value_page_pv"></span></div></div></div></div><div class="layout layout_post" id="content-inner">   <article id="post"><div class="article-container" id="post-content"><h2 id="CNN最近几年的发展"><a href="#CNN最近几年的发展" class="headerlink" title="CNN最近几年的发展"></a>CNN最近几年的发展</h2><p>​        CNN，全称为<em>Convolutional</em> <em>Neural</em> <em>Network</em>，中文名称卷积神经网络。</p>
<p>​        卷积神经网络（Convolutional Neural Networks, CNN）是一类包含<a href="https://baike.baidu.com/item/卷积/9411006" target="_blank" rel="noopener">卷积</a>计算且具有深度结构的<a href="https://baike.baidu.com/item/前馈神经网络/7580523" target="_blank" rel="noopener">前馈神经网络</a>（Feedforward Neural Networks），是<a href="https://baike.baidu.com/item/深度学习/3729729" target="_blank" rel="noopener">深度学习</a>（deep learning）的代表算法之一   。卷积神经网络具有<a href="https://baike.baidu.com/item/表征学习/2140515" target="_blank" rel="noopener">表征学习</a>（representation learning）能力，能够按其阶层结构对输入信息进行平移不变分类（shift-invariant classification），因此也被称为“平移不变人工神经网络（Shift-Invariant Artificial Neural Networks, SIANN）” 。</p>
<p>​        对卷积神经网络的研究始于二十世纪80至90年代，时间延迟网络和LeNet-5是最早出现的卷积神经网络  ；在二十一世纪后，随着深度学习理论的提出和数值计算设备的改进，卷积神经网络得到了快速发展，并被应用于<a href="https://baike.baidu.com/item/计算机视觉/2803351" target="_blank" rel="noopener">计算机视觉</a>、<a href="https://baike.baidu.com/item/自然语言处理/365730" target="_blank" rel="noopener">自然语言处理</a>等领域 。</p>
<p>​        <img alt data-src="https://s1.ax1x.com/2020/03/15/88wn3Q.png" class="lazyload"></p>
<p>​       在2012年之前，ImageNet的错误率都是比较高的，直到2012年AlexNet的横空出世，将错误率一下子下降了</p>
<p>近10%，掀起了一股深度学习的浪潮。</p>
<h3 id="AlexNet"><a href="#AlexNet" class="headerlink" title="AlexNet"></a>AlexNet</h3><p>​     <img alt data-src="https://s1.ax1x.com/2020/03/15/88wlBq.png" class="lazyload"></p>
<p>​        如图为Alexnet的网络结构，它是由八层网络构成，以11*11的kernel进行大刀阔斧的提取特征，在当时是很很好的模型，在今天看来，它的粒度太大。可以看到，当时比较先进的显卡只有3GB大小，所以用两块显卡跑。</p>
<h3 id="VGG"><a href="#VGG" class="headerlink" title="VGG"></a>VGG</h3><p>​        <img alt data-src="https://s1.ax1x.com/2020/03/15/88w8EV.png" class="lazyload"></p>
<p>​        这是2014年ILSVRC的亚军VGG，相比2012年的AlexNet，error又下降了近10%。它的网络层数又加深了，从AlexNet的8层网络，变成了11-19层。Kernel大小也变得更加细致了。</p>
<h3 id="GoogleNET"><a href="#GoogleNET" class="headerlink" title="GoogleNET"></a>GoogleNET</h3><p>​    <img alt data-src="https://s1.ax1x.com/2020/03/15/88wGNT.png" class="lazyload"></p>
<p>​        上图为2014年的冠军GoogLeNet,为了纪念1998年Yann LeCun提出的1998年LeNet5,将字母L大写。该网络模型为22层。当时人们有一个想法，是不是网络层数越多，train的效果越好呢？在经历大量的实验之后，发现并不是这样的。也就是说，并不是层数越多越好。于是中国学者何凯明（大神一位，有兴趣可以百度）在2015年提出了一种网络模型叫做残差网络ResNet。</p>
<p>​        该模型的创新点在于，它提出了一个类似于电路中“短接”的概念，将那些train效果起到negative作用的layer丢掉，从而在保证层数的情况下又保证了一个low error。</p>
<p>​        <img alt data-src="https://s1.ax1x.com/2020/03/15/882lz6.png" class="lazyload"></p>
<p>我们今天主要介绍LeNet5和ResNet。</p>
<p>​    </p>
<h2 id="LeNet5"><a href="#LeNet5" class="headerlink" title="LeNet5"></a>LeNet5</h2><p>​        LeNet5是1998年由Yann LeCun及其团队提出的，该模型在当时的手写数字识别问题中取得了成功。</p>
<p><img alt data-src="https://s1.ax1x.com/2020/03/15/88wucj.png" class="lazyload"></p>
<p>​    该网络由2层卷积层，3层全连接层共5层网络构成。图片的大小为32*32.</p>
<h4 id="lenet5-py"><a href="#lenet5-py" class="headerlink" title="lenet5.py"></a>lenet5.py</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Lenet5</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super(Lenet5, self).__init__()</span><br><span class="line">        self.conv_unit = nn.Sequential(</span><br><span class="line">            <span class="comment">#x :[b,3,32,32]</span></span><br><span class="line">            nn.Conv2d(in_channels=<span class="number">3</span>,out_channels=<span class="number">6</span>,kernel_size=<span class="number">5</span>,stride=<span class="number">1</span>,padding=<span class="number">0</span>),<span class="comment">#新建卷积层</span></span><br><span class="line">            nn.MaxPool2d(kernel_size=<span class="number">2</span>,stride=<span class="number">2</span>,padding=<span class="number">0</span>), <span class="comment">#池化层压缩特征 只改变长款，不改变channel</span></span><br><span class="line">            nn.Conv2d(in_channels=<span class="number">6</span>,out_channels=<span class="number">16</span>,kernel_size=<span class="number">5</span>,stride=<span class="number">1</span>,padding=<span class="number">0</span>),</span><br><span class="line">            nn.MaxPool2d(kernel_size=<span class="number">2</span>,stride=<span class="number">2</span>,padding=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">        )</span><br><span class="line">        self.fc_unit = nn.Sequential( <span class="comment">#三成全连接层</span></span><br><span class="line">            nn.Linear(<span class="number">16</span>*<span class="number">5</span>*<span class="number">5</span>,<span class="number">120</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Linear(<span class="number">120</span>,<span class="number">84</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Linear(<span class="number">84</span>,<span class="number">10</span>)</span><br><span class="line">        )</span><br><span class="line">        <span class="comment">#[b,3,32,32]</span></span><br><span class="line">        tmp = torch.randn(<span class="number">2</span>,<span class="number">3</span>,<span class="number">32</span>,<span class="number">32</span>)</span><br><span class="line">        out = self.conv_unit(tmp)</span><br><span class="line">        <span class="comment">#[b,16,5,5]</span></span><br><span class="line">        print(<span class="string">'conv_out:'</span>,out.shape)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        batchsz = x.size(<span class="number">0</span>)</span><br><span class="line">        <span class="comment">#[b,3,32,32] =&gt; [b,16,5,5]</span></span><br><span class="line">        x = self.conv_unit(x)</span><br><span class="line">        <span class="comment">#[b,16,5,5] =&gt; [b,16*5*5]</span></span><br><span class="line">        x = x.view(batchsz,<span class="number">16</span>*<span class="number">5</span>*<span class="number">5</span>) <span class="comment">#flatten</span></span><br><span class="line">        <span class="comment">#[b,16*5*5] =&gt; [b,10]</span></span><br><span class="line">        logits = self.fc_unit(x)</span><br><span class="line">        <span class="keyword">return</span> logits</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">()</span>:</span></span><br><span class="line">    net = Lenet5()</span><br><span class="line">    tmp = torch.randn(<span class="number">2</span>,<span class="number">3</span>,<span class="number">32</span>,<span class="number">32</span>) </span><br><span class="line">    out = net(tmp)</span><br><span class="line">    print(<span class="string">'lenet out:'</span>,out.shape)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure>

<h4 id="main-py"><a href="#main-py" class="headerlink" title="main.py"></a>main.py</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> datasets</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span>  nn,optim</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> Lenet5 <span class="keyword">import</span> Lenet5</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">()</span>:</span></span><br><span class="line">    batchsz=<span class="number">128</span>   <span class="comment">#根据自己机器的性能来定义batchsize，不建议太小，因为梯度的优化方向是按照平均</span></span><br><span class="line">    			  <span class="comment">#梯度方向，所以太小会具有偶然性</span></span><br><span class="line">                <span class="comment">#新建cifar文件夹，train = true ，transforms</span></span><br><span class="line">    cifar_train = datasets.CIFAR10(<span class="string">'cifar'</span>,<span class="literal">True</span>,transform=transforms.Compose([</span><br><span class="line">        transforms.Resize((<span class="number">32</span>,<span class="number">32</span>)), <span class="comment">#resize程需要的大小</span></span><br><span class="line">        transforms.ToTensor()</span><br><span class="line">    ]),download=<span class="literal">True</span>)				<span class="comment">#装载CIFAR10数据集</span></span><br><span class="line"></span><br><span class="line">    cifar_train = DataLoader(cifar_train,batch_size=batchsz,shuffle=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    cifar_test = datasets.CIFAR10(<span class="string">'cifar'</span>, <span class="literal">True</span>, transform=transforms.Compose([</span><br><span class="line">        transforms.Resize((<span class="number">32</span>,<span class="number">32</span>)),  <span class="comment"># resize程需要的大小</span></span><br><span class="line">        transforms.ToTensor()</span><br><span class="line">    ]), download=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    cifar_test = DataLoader(cifar_test, batch_size=batchsz, shuffle=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    x,label = iter(cifar_train).next()</span><br><span class="line">    print(<span class="string">'x:'</span>,x.shape,<span class="string">'label:'</span>,label.shape)</span><br><span class="line"></span><br><span class="line">    device = torch.device(<span class="string">'cuda'</span>)  <span class="comment">#指定GPU寻来你</span></span><br><span class="line">    model = Lenet5().to(device)</span><br><span class="line">    criteon = nn.CrossEntropyLoss().to(device)</span><br><span class="line">    optimizer = optim.Adam(model.parameters(),lr = <span class="number">1e-3</span>)</span><br><span class="line">    print(model)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> range(<span class="number">100</span>):</span><br><span class="line">        model.train()</span><br><span class="line">        <span class="keyword">for</span> batchidx,(x,label) <span class="keyword">in</span> enumerate(cifar_train):</span><br><span class="line">            <span class="comment">#x [b,3,32,32]</span></span><br><span class="line">            <span class="comment">#label [b]</span></span><br><span class="line">            x,label = x.to(device),label.to(device)</span><br><span class="line">            logits = model(x)</span><br><span class="line">            <span class="comment">#logits:[b,10]</span></span><br><span class="line">            <span class="comment">#label: [b]</span></span><br><span class="line">            <span class="comment">#loss:tensor scalar</span></span><br><span class="line">            loss = criteon(logits,label)</span><br><span class="line"></span><br><span class="line">            <span class="comment">#backpropagate</span></span><br><span class="line">            optimizer.zero_grad()      <span class="comment">#记得迭代之前梯度要清零</span></span><br><span class="line">            loss.backward()</span><br><span class="line">            optimizer.step()</span><br><span class="line"></span><br><span class="line">        print(epoch,<span class="string">"single_loss:"</span>,loss.item()) <span class="comment">#打印一个batch的loss</span></span><br><span class="line"></span><br><span class="line">        model.eval()            <span class="comment">#验证集</span></span><br><span class="line">        <span class="keyword">with</span> torch.no_grad(): <span class="comment">#加入此函数的目的是为了不要带有梯度，因为测试集不需要梯度</span></span><br><span class="line">            <span class="comment">#test</span></span><br><span class="line">            total_correct = <span class="number">0</span></span><br><span class="line">            total_num = <span class="number">0</span></span><br><span class="line">            <span class="keyword">for</span> x,label <span class="keyword">in</span> cifar_test:</span><br><span class="line">                <span class="comment">#[b,3,32,32]</span></span><br><span class="line">                <span class="comment">#[b]</span></span><br><span class="line">                x,label = x.to(device),label.to(device)</span><br><span class="line"></span><br><span class="line">                <span class="comment">#[b,10]</span></span><br><span class="line">                logits = model(x)</span><br><span class="line">                <span class="comment">#[b]</span></span><br><span class="line">                pred = logits.argmax(dim = <span class="number">1</span>) <span class="comment">#max 取最大值 argmax取最大值所对应的下标</span></span><br><span class="line">                <span class="comment">#[b] vs[b] =&gt; scalar tensor</span></span><br><span class="line">                correct = torch.eq(pred,label).float().sum().item()</span><br><span class="line">                total_correct += correct</span><br><span class="line">                total_num += x.size(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">            acc = total_correct/total_num</span><br><span class="line">            print(epoch,<span class="string">'test acc:'</span>,acc)   <span class="comment">#打印平均正确率</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure>

<p>​        可以看到，在epoch达到50多次的时候会出现明显的震荡。</p>
<p><img alt data-src="https://s1.ax1x.com/2020/03/15/88w1H0.png" class="lazyload"></p>
<p>​     我仅仅训练了100个epoch,accuracy达到了97%左右</p>
<p>​                <img alt data-src="https://s1.ax1x.com/2020/03/15/88wQun.png" class="lazyload"></p>
<h2 id="ResNet"><a href="#ResNet" class="headerlink" title="ResNet"></a>ResNet</h2><p>​            </p>
<p><img alt data-src="https://s1.ax1x.com/2020/03/15/882lz6.png" class="lazyload"></p>
<p>​        以这样的”短接”操作，来保证层数和准确率。ResNet是何凯明同学在2015年的ILSVRC提出的，同时也是这届大赛的冠军。他于2016年获得CVPR的Best Paper。</p>
<h4 id="ResNet-py"><a href="#ResNet-py" class="headerlink" title="ResNet.py"></a>ResNet.py</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span>  torch</span><br><span class="line"><span class="keyword">from</span>    torch <span class="keyword">import</span>  nn</span><br><span class="line"><span class="keyword">from</span>    torch.nn <span class="keyword">import</span> functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ResBlk</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    resnet block</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, ch_in, ch_out, stride=<span class="number">1</span>)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        :param ch_in:</span></span><br><span class="line"><span class="string">        :param ch_out:</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        super(ResBlk, self).__init__()</span><br><span class="line"></span><br><span class="line">        </span><br><span class="line">        self.conv1 = nn.Conv2d(ch_in, ch_out, kernel_size=<span class="number">3</span>, stride=stride, padding=<span class="number">1</span>)</span><br><span class="line">        self.bn1 = nn.BatchNorm2d(ch_out)  <span class="comment">#序列化  让train更加的快速和稳定</span></span><br><span class="line">        self.conv2 = nn.Conv2d(ch_out, ch_out, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>)</span><br><span class="line">        self.bn2 = nn.BatchNorm2d(ch_out)</span><br><span class="line"></span><br><span class="line">        self.extra = nn.Sequential()  <span class="comment">#初始化self.extra</span></span><br><span class="line">        <span class="keyword">if</span> ch_out != ch_in: <span class="comment">#如果输入输出的channel不匹配，进行归一</span></span><br><span class="line">            <span class="comment"># [b, ch_in, h, w] =&gt; [b, ch_out, h, w]</span></span><br><span class="line">            self.extra = nn.Sequential(</span><br><span class="line">                nn.Conv2d(ch_in, ch_out, kernel_size=<span class="number">1</span>, stride=stride),</span><br><span class="line">                nn.BatchNorm2d(ch_out)</span><br><span class="line">            )</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        :param x: [b, ch, h, w]</span></span><br><span class="line"><span class="string">        :return:</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        out = F.relu(self.bn1(self.conv1(x)))</span><br><span class="line">        out = self.bn2(self.conv2(out))   <span class="comment">#激活函数可加可不加</span></span><br><span class="line">        <span class="comment"># short cut.</span></span><br><span class="line">        <span class="comment"># extra module: [b, ch_in, h, w] =&gt; [b, ch_out, h, w]</span></span><br><span class="line">        <span class="comment"># element-wise add:</span></span><br><span class="line">        out = self.extra(x) + out  <span class="comment">#维度不匹配则无法相加，对应于图片中的F(X)+X</span></span><br><span class="line">        out = F.relu(out)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ResNet18</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super(ResNet18, self).__init__()</span><br><span class="line"></span><br><span class="line">        self.conv1 = nn.Sequential(</span><br><span class="line">            nn.Conv2d(<span class="number">3</span>, <span class="number">64</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">3</span>, padding=<span class="number">0</span>),</span><br><span class="line">            nn.BatchNorm2d(<span class="number">64</span>)</span><br><span class="line">        )</span><br><span class="line">        <span class="comment"># followed 4 blocks</span></span><br><span class="line">        <span class="comment"># [b, 64, h, w] =&gt; [b, 128, h ,w]</span></span><br><span class="line">        self.blk1 = ResBlk(<span class="number">64</span>, <span class="number">128</span>, stride=<span class="number">2</span>)</span><br><span class="line">        <span class="comment"># [b, 128, h, w] =&gt; [b, 256, h, w]</span></span><br><span class="line">        self.blk2 = ResBlk(<span class="number">128</span>, <span class="number">256</span>, stride=<span class="number">2</span>)</span><br><span class="line">        <span class="comment"># # [b, 256, h, w] =&gt; [b, 512, h, w]</span></span><br><span class="line">        self.blk3 = ResBlk(<span class="number">256</span>, <span class="number">512</span>, stride=<span class="number">2</span>)</span><br><span class="line">        <span class="comment"># # [b, 512, h, w] =&gt; [b, 1024, h, w]</span></span><br><span class="line">        self.blk4 = ResBlk(<span class="number">512</span>, <span class="number">512</span>, stride=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">        self.outlayer = nn.Linear(<span class="number">512</span>*<span class="number">1</span>*<span class="number">1</span>, <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        :param x:</span></span><br><span class="line"><span class="string">        :return:</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        x = F.relu(self.conv1(x))</span><br><span class="line">        <span class="comment"># [b, 64, h, w] =&gt; [b, 1024, h, w]</span></span><br><span class="line">        x = self.blk1(x)</span><br><span class="line">        x = self.blk2(x)</span><br><span class="line">        x = self.blk3(x)</span><br><span class="line">        x = self.blk4(x)</span><br><span class="line">        <span class="comment"># print('after conv:', x.shape) #[b, 512, 2, 2]</span></span><br><span class="line">        <span class="comment"># [b, 512, h, w] =&gt; [b, 512, 1, 1]</span></span><br><span class="line">        x = F.adaptive_avg_pool2d(x, [<span class="number">1</span>, <span class="number">1</span>])</span><br><span class="line">        <span class="comment"># print('after pool:', x.shape)</span></span><br><span class="line">        x = x.view(x.size(<span class="number">0</span>), <span class="number">-1</span>)</span><br><span class="line">        x = self.outlayer(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">()</span>:</span></span><br><span class="line">    blk = ResBlk(<span class="number">64</span>, <span class="number">128</span>, stride=<span class="number">4</span>)</span><br><span class="line">    tmp = torch.randn(<span class="number">2</span>, <span class="number">64</span>, <span class="number">32</span>, <span class="number">32</span>)</span><br><span class="line">    out = blk(tmp)</span><br><span class="line">    print(<span class="string">'block:'</span>, out.shape)</span><br><span class="line">    x = torch.randn(<span class="number">2</span>, <span class="number">3</span>, <span class="number">32</span>, <span class="number">32</span>)</span><br><span class="line">    model = ResNet18()</span><br><span class="line">    out = model(x)</span><br><span class="line">    print(<span class="string">'resnet:'</span>, out.shape)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#main.py</span></span><br><span class="line"><span class="keyword">import</span>  torch</span><br><span class="line"><span class="keyword">from</span>    torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"><span class="keyword">from</span>    torchvision <span class="keyword">import</span> datasets</span><br><span class="line"><span class="keyword">from</span>    torchvision <span class="keyword">import</span> transforms</span><br><span class="line"><span class="keyword">from</span>    torch <span class="keyword">import</span> nn, optim</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span>    lenet5 <span class="keyword">import</span> Lenet5</span><br><span class="line"><span class="keyword">from</span>    resnet <span class="keyword">import</span> ResNet18</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">()</span>:</span></span><br><span class="line">    batchsz = <span class="number">128</span></span><br><span class="line"></span><br><span class="line">    cifar_train = datasets.CIFAR10(<span class="string">'cifar'</span>, <span class="literal">True</span>, transform=transforms.Compose([</span><br><span class="line">        transforms.Resize((<span class="number">32</span>, <span class="number">32</span>)),</span><br><span class="line">        transforms.ToTensor(),</span><br><span class="line">        transforms.Normalize(mean=[<span class="number">0.485</span>, <span class="number">0.456</span>, <span class="number">0.406</span>], <span class="comment">#RGB三个通道的均值</span></span><br><span class="line">                             std=[<span class="number">0.229</span>, <span class="number">0.224</span>, <span class="number">0.225</span>]) <span class="comment">#RGB三个通道的方差</span></span><br><span class="line">    ]), download=<span class="literal">True</span>)</span><br><span class="line">    cifar_train = DataLoader(cifar_train, batch_size=batchsz, shuffle=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    cifar_test = datasets.CIFAR10(<span class="string">'cifar'</span>, <span class="literal">False</span>, transform=transforms.Compose([</span><br><span class="line">        transforms.Resize((<span class="number">32</span>, <span class="number">32</span>)),</span><br><span class="line">        transforms.ToTensor(),</span><br><span class="line">        transforms.Normalize(mean=[<span class="number">0.485</span>, <span class="number">0.456</span>, <span class="number">0.406</span>],</span><br><span class="line">                             std=[<span class="number">0.229</span>, <span class="number">0.224</span>, <span class="number">0.225</span>])</span><br><span class="line">    ]), download=<span class="literal">True</span>)</span><br><span class="line">    cifar_test = DataLoader(cifar_test, batch_size=batchsz, shuffle=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    x, label = iter(cifar_train).next()</span><br><span class="line">    print(<span class="string">'x:'</span>, x.shape, <span class="string">'label:'</span>, label.shape)</span><br><span class="line"></span><br><span class="line">    device = torch.device(<span class="string">'cuda'</span>)</span><br><span class="line">    <span class="comment"># model = Lenet5().to(device)</span></span><br><span class="line">    model = ResNet18().to(device)</span><br><span class="line"></span><br><span class="line">    criteon = nn.CrossEntropyLoss().to(device)</span><br><span class="line">    optimizer = optim.Adam(model.parameters(), lr=<span class="number">1e-3</span>)</span><br><span class="line">    print(model)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> range(<span class="number">1000</span>):</span><br><span class="line"></span><br><span class="line">        model.train()</span><br><span class="line">        <span class="keyword">for</span> batchidx, (x, label) <span class="keyword">in</span> enumerate(cifar_train):</span><br><span class="line">            <span class="comment"># [b, 3, 32, 32]</span></span><br><span class="line">            <span class="comment"># [b]</span></span><br><span class="line">            x, label = x.to(device), label.to(device)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">            logits = model(x)</span><br><span class="line">            <span class="comment"># logits: [b, 10]</span></span><br><span class="line">            <span class="comment"># label:  [b]</span></span><br><span class="line">            <span class="comment"># loss: tensor scalar</span></span><br><span class="line">            loss = criteon(logits, label)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># backprop</span></span><br><span class="line">            optimizer.zero_grad()</span><br><span class="line">            loss.backward()</span><br><span class="line">            optimizer.step()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        print(epoch, <span class="string">'loss:'</span>, loss.item())</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        model.eval()</span><br><span class="line">        <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">            <span class="comment"># test</span></span><br><span class="line">            total_correct = <span class="number">0</span></span><br><span class="line">            total_num = <span class="number">0</span></span><br><span class="line">            <span class="keyword">for</span> x, label <span class="keyword">in</span> cifar_test:</span><br><span class="line">                <span class="comment"># [b, 3, 32, 32]</span></span><br><span class="line">                <span class="comment"># [b]</span></span><br><span class="line">                x, label = x.to(device), label.to(device)</span><br><span class="line"></span><br><span class="line">                <span class="comment"># [b, 10]</span></span><br><span class="line">                logits = model(x)</span><br><span class="line">                <span class="comment"># [b]</span></span><br><span class="line">                pred = logits.argmax(dim=<span class="number">1</span>)</span><br><span class="line">                <span class="comment"># [b] vs [b] =&gt; scalar tensor</span></span><br><span class="line">                correct = torch.eq(pred, label).float().sum().item()</span><br><span class="line">                total_correct += correct</span><br><span class="line">                total_num += x.size(<span class="number">0</span>)</span><br><span class="line">                <span class="comment"># print(correct)</span></span><br><span class="line"></span><br><span class="line">            acc = total_correct / total_num</span><br><span class="line">            print(epoch, <span class="string">'test acc:'</span>, acc)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure>

<p>​        可以看到，ResNet的main文件和LeNet5的main文件相差不多。这说明，我门可以一套模板来训练不同的模型。实际应用中还可以加入一些工程技巧，比如数据增强操作，图片的旋转角度不宜太大，在-15°到15°为宜，角度太大，经实验证明，效果并不好。</p>
</div></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">Author: </span><span class="post-copyright-info"><a href="mailto:undefined">superzhaoyang</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">Link: </span><span class="post-copyright-info"><a href="http://yoursite.com/2020/03/15/卷积神经网络实战之Lenet5-Resnet/">http://yoursite.com/2020/03/15/卷积神经网络实战之Lenet5-Resnet/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">Copyright Notice: </span><span class="post-copyright-info">All articles in this blog are licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> unless stating additionally.</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/deep-learning/">deep learning    </a><a class="post-meta__tags" href="/tags/CNN/">CNN    </a><a class="post-meta__tags" href="/tags/Lenet5/">Lenet5    </a><a class="post-meta__tags" href="/tags/Resnet/">Resnet    </a></div><div class="post_share"><div class="social-share" data-image="https://s1.ax1x.com/2020/03/15/88wucj.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js@1.0.16/dist/css/share.min.css"><script src="https://cdn.jsdelivr.net/npm/social-share.js@1.0.16/dist/js/social-share.min.js"></script></div></div><div class="post-reward"><a class="reward-button"><i class="fa fa-qrcode"></i> Donate<div class="reward-main"><ul class="reward-all"><li class="reward-item"><img class="lazyload post-qr-code__img" src="/img/wechat.jpg"><div class="post-qr-code__desc">微信</div></li><li class="reward-item"><img class="lazyload post-qr-code__img" src="/img/alipay.jpg"><div class="post-qr-code__desc">支付宝</div></li></ul></div></a></div><nav class="pagination_post" id="pagination"><div class="prev-post pull_left"><a href="/2020/03/16/关于Ubuntu19.10下安装nginx遇到的问题/"><img class="prev_cover lazyload" data-src="https://s1.ax1x.com/2020/03/16/8JWHRs.png" onerror="onerror=null;src='/img/404.jpg'"><div class="label">Previous Post</div><div class="prev_info"><span>关于Ubuntu 19.10安装nginx时遇到的问题</span></div></a></div><div class="next-post pull_right"><a href="/2020/03/15/github实用搜索技巧/"><img class="next_cover lazyload" data-src="https://cdn.sspai.com/2018/08/10/8fff67288a075c18b3ddad2e67b2abbf.jpg?imageView2/2/w/1120/q/90/interlace/1/ignore-error/1" onerror="onerror=null;src='/img/404.jpg'"><div class="label">Next Post</div><div class="next_info"><span>github实用搜索技巧</span></div></a></div></nav><div class="relatedPosts"><div class="relatedPosts_headline"><i class="fa fa-fw fa-thumbs-up" aria-hidden="true"></i><span> Recommend</span></div><div class="relatedPosts_list"><div class="relatedPosts_item"><a href="/2020/03/22/循环神经网络/" title="循环神经网络RNN"><img class="relatedPosts_cover lazyload" data-src="https://s1.ax1x.com/2020/03/22/84fiEd.png"><div class="relatedPosts_title">循环神经网络RNN</div></a></div></div><div class="clear_both"></div></div><hr><div id="post-comment"><div class="comment_headling"><i class="fa fa-comments fa-fw" aria-hidden="true"></i><span> Comment</span></div><div class="vcomment" id="vcomment"></div><script src="https://cdn.jsdelivr.net/npm/valine/dist/Valine.min.js"></script><script>var notify = false == true ? true : false;
var verify = false == true ? true : false;
var GUEST_INFO = ['nick','mail','link'];
var guest_info = 'nick,mail,link'.split(',').filter(function(item){
  return GUEST_INFO.indexOf(item) > -1
});
guest_info = guest_info.length == 0 ? GUEST_INFO :guest_info;

window.valine = new Valine({
  el:'#vcomment',
  notify:notify,
  verify:verify,
  appId:'REItCuJxdP5eRCIkGnFNRGWt-gzGzoHsz',
  appKey:'CdrBY5eDdonSzQutnKqm0fBv',
  placeholder:'Please leave your footprints',
  avatar:'monsterid',
  guest_info:guest_info,
  pageSize:'10',
  lang:'en',
  recordIP: true
});</script></div></div></div><footer id="footer" style="background-image: url(https://s1.ax1x.com/2020/03/15/88wucj.png)"><div id="footer-wrap"><div class="copyright">&copy;2019 - 2020 By superzhaoyang</div><div class="framework-info"><span>Driven </span><a href="http://hexo.io"><span>Hexo</span></a><span class="footer-separator">|</span><span>Theme </span><a href="https://github.com/jerryc127/hexo-theme-butterfly"><span>Butterfly</span></a></div><div class="footer_custom_text">Hi,  welcome  to  my  <a href="https://superzhaoyang.top/">blog</a>!</div></div></footer></div><section class="rightside" id="rightside"><div id="rightside-config-hide"><i class="fa fa-book" id="readmode" title="Read Mode"></i><i class="fa fa-plus" id="font_plus" title="Increase font size"></i><i class="fa fa-minus" id="font_minus" title="Decrease font size"></i><a class="translate_chn_to_cht" id="translateLink" href="javascript:translatePage();" title="Traditional Chinese and Simplified Chinese Conversion" target="_self">繁</a><i class="darkmode fa fa-moon-o" id="darkmode" title="Dark Mode"></i></div><div id="rightside-config-show"><div id="rightside_config" title="Setting"><i class="fa fa-cog" aria-hidden="true"></i></div><a id="to_comment" href="#post-comment" title="Scroll to comment"><i class="scroll_to_comment fa fa-comments">  </i></a><i class="fa fa-list-ul close" id="mobile-toc-button" title="Table of Contents" aria-hidden="true"></i><i class="fa fa-arrow-up" id="go-up" title="Back to top" aria-hidden="true"></i></div></section><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/medium-zoom/dist/medium-zoom.min.js"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
    processEscapes: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
  },
  CommonHTML: {
    linebreaks: { automatic: true, width: "90% container" }
  },
  "HTML-CSS": { 
    linebreaks: { automatic: true, width: "90% container" }
  },
  "SVG": { 
    linebreaks: { automatic: true, width: "90% container" }
  }
});
</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
  var all = MathJax.Hub.getAllJax(), i;
  for (i=0; i < all.length; i += 1) {
    all[i].SourceElement().parentNode.className += ' has-jax';
  }
});
</script><script src="https://cdn.jsdelivr.net/npm/mathjax/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script async src="/js/search/local-search.js"></script><script src="https://cdn.jsdelivr.net/npm/animejs@latest/anime.min.js"></script><script src="/js/third-party/fireworks.js"></script><script src="https://cdn.jsdelivr.net/npm/node-snackbar@0.1.15/dist/snackbar.min.js"></script><script id="ribbon" src="/js/third-party/canvas-ribbon.js" size="150" alpha="0.6" zindex="-1" mobile="true" data-click="true"></script><script id="ribbon_piao" mobile="true" src="/js/third-party/piao.js"></script><script id="canvas_nest" color="0,0,255" opacity="0.7" zindex="-1" count="99" mobile="false" src="/js/third-party/canvas-nest.js"></script><script src="/js/baidupush.js"> </script><script src="/js/third-party/activate-power-mode.js"></script><script>POWERMODE.colorful = true; // make power mode colorful
POWERMODE.shake = true; // turn off shake
document.body.addEventListener('input', POWERMODE);
</script><script src="/js/tw_cn.js"></script><script>translateInitilization()
</script><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><script src="https://cdn.jsdelivr.net/npm/instant.page@latest/instantpage.min.js" type="module"></script><script src="https://cdn.jsdelivr.net/npm/lazysizes@latest/lazysizes.min.js" async></script><script src="/js/third-party/click_heart.js"></script><script src="/js/third-party/ClickShowText.js"></script><div class="search-dialog" id="local-search"><div class="search-dialog__title" id="local-search-title">Local search</div><div id="local-input-panel"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="Search for Posts"></div></div></div><hr><div id="local-search-results"><div id="local-hits"></div><div id="local-stats"><div class="local-search-stats__hr" id="hr"><span>Powered by</span> <a href="https://github.com/wzpan/hexo-generator-search" style="color:#49B1F5;">hexo-generator-search</a></div></div></div><span class="search-close-button"><i class="fa fa-times"></i></span></div><div class="search-mask"></div></body></html>