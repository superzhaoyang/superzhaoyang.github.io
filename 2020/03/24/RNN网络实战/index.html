<!DOCTYPE html><html lang="zh-Hans" data-theme="light"><head><meta name="generator" content="Hexo 3.9.0"><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><title>RNN网络实战 | superzhaoyang</title><meta name="description" content="基于pytorch的RNN网络实战"><meta name="keywords" content="循环神经网络,RNN,pytorch"><meta name="author" content="superzhaoyang"><meta name="copyright" content="superzhaoyang"><meta name="format-detection" content="telephone=no"><link rel="shortcut icon" href="/img/favicon.ico"><link rel="preconnect" href="//cdn.jsdelivr.net"><meta name="twitter:card" content="summary_large_image"><meta name="twitter:title" content="RNN网络实战"><meta name="twitter:description" content="基于pytorch的RNN网络实战"><meta name="twitter:image" content="https://s1.ax1x.com/2020/03/24/8b2SkF.png"><meta property="og:type" content="article"><meta property="og:title" content="RNN网络实战"><meta property="og:url" content="http://yoursite.com/2020/03/24/RNN网络实战/"><meta property="og:site_name" content="superzhaoyang"><meta property="og:description" content="基于pytorch的RNN网络实战"><meta property="og:image" content="https://s1.ax1x.com/2020/03/24/8b2SkF.png"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><script src="https://cdn.jsdelivr.net/npm/js-cookie/dist/js.cookie.min.js"></script><script>const autoChangeMode = 'true'
var t = Cookies.get("theme");
if (autoChangeMode == '1'){
const isDarkMode = window.matchMedia("(prefers-color-scheme: dark)").matches
const isLightMode = window.matchMedia("(prefers-color-scheme: light)").matches
const isNotSpecified = window.matchMedia("(prefers-color-scheme: no-preference)").matches
const hasNoSupport = !isDarkMode && !isLightMode && !isNotSpecified

if (t === undefined){
  if (isLightMode) activateLightMode()
  else if (isDarkMode) activateDarkMode()
  else if (isNotSpecified || hasNoSupport){
    console.log('You specified no preference for a color scheme or your browser does not support it. I Schedule dark mode during night time.')
    now = new Date();
    hour = now.getHours();
    isNight = hour < 6 || hour >= 18
    isNight ? activateDarkMode() : activateLightMode()
}
} else if (t == 'light') activateLightMode()
else activateDarkMode()


} else if (autoChangeMode == '2'){
  now = new Date();
  hour = now.getHours();
  isNight = hour < 6 || hour >= 18
  if(t === undefined) isNight? activateDarkMode() : activateLightMode()
  else if (t === 'light') activateLightMode()
  else activateDarkMode() 
} else {
  if ( t == 'dark' ) activateDarkMode()
  else if ( t == 'light') activateLightMode()
}

function activateDarkMode(){
  document.documentElement.setAttribute('data-theme', 'dark')
  if (document.querySelector('meta[name="theme-color"]') !== null){
    document.querySelector('meta[name="theme-color"]').setAttribute('content','#000')
  }
}
function activateLightMode(){
  document.documentElement.setAttribute('data-theme', 'light')
  if (document.querySelector('meta[name="theme-color"]') !== null){
  document.querySelector('meta[name="theme-color"]').setAttribute('content','#fff')
  }
}</script><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="/fonts/font-awesome.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/node-snackbar@0.1.15/dist/snackbar.min.css"><link rel="canonical" href="http://yoursite.com/2020/03/24/RNN网络实战/"><link rel="prev" title="数据结构课设实验五s之山威地图" href="http://yoursite.com/2020/03/25/数据结构课设实验五之山威地图/"><link rel="next" title="Typora快捷键" href="http://yoursite.com/2020/03/24/typora快捷键/"><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Titillium+Web"><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"search.xml","languages":{"hits_empty":"We didn't find any results for the search: ${query}"}},
  translate: {"defaultEncoding":2,"translateDelay":0,"cookieDomain":"https://www.superzhaoyang.top/","msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"简"},
  highlight_copy: 'true',
  highlight_lang: 'true',
  highlight_shrink: 'false',
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  },
  bookmark: {
    title: 'Snackbar.bookmark.title',
    message_prev: 'Press',
    message_next: 'to bookmark this page'
  },
  runtime_unit: 'days',
  copyright: undefined,
  copy_copyright_js: false,
  ClickShowText: {"text":"superzhaoyang,帅,聪明,IT,敲代码,LOL,玩,酷,绚,NB,乒乓球,付同有我儿子","fontSize":"15px"},
  medium_zoom: 'true',
  Snackbar: {"bookmark":{"title":"Snackbar.bookmark.title","message_prev":"Press","message_next":"to bookmark this page"},"chs_to_cht":"Traditional Chinese Activated Manually","cht_to_chs":"Simplified Chinese Activated Manually","day_to_night":"Light Mode Activated Manually","night_to_day":"Dark Mode Activated Manually","bgLight":"#49b1f5","bgDark":"#2d3035","position":"bottom-left"}
  
}</script></head><body><canvas class="fireworks"></canvas><div id="header"> <div id="page-header"><span class="pull_left" id="blog_name"><a class="blog_title" id="site-name" href="/">superzhaoyang</a></span><i class="fa fa-bars fa-fw toggle-menu pull_right close" aria-hidden="true"></i><span class="pull_right menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fa fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fa fa-archive"></i><span> 档案</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fa fa-folder-open"></i><span> 类别</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fa fa-link"></i><span> 链接</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fa fa-heart"></i><span> 关于</span></a></div><div class="menus_item"><a class="site-page"><i class="fa-fw fa fa-list" aria-hidden="true"></i><span> 学习和娱乐</span><i class="fa fa-chevron-down menus-expand" aria-hidden="true"></i></a><ul class="menus_item_child"><li><a class="site-page" href="/music/"><i class="fa-fw fa fa-music"></i><span> 音乐</span></a></li><li><a class="site-page" href="/movies/"><i class="fa-fw fa fa-film"></i><span> 电影</span></a></li><li><a class="site-page" href="/book/"><i class="fa-fw fa fa-book"></i><span> 书籍</span></a></li></ul></div></div></span><span class="pull_right" id="search_button"><a class="site-page social-icon search"><i class="fa fa-search fa-fw"></i><span> Search</span></a></span></div></div><div id="mobile-sidebar"><div id="menu_mask"></div><div id="mobile-sidebar-menus"><div class="mobile_author_icon"><img class="lazyload avatar_img" src="https://s2.ax1x.com/2019/09/10/naEP1g.jpg" onerror="onerror=null;src='/img/friend_404.gif'"></div><div class="mobile_post_data"><div class="mobile_data_item is_center"><div class="mobile_data_link"><a href="/archives/"><div class="headline">Articles</div><div class="length_num">50</div></a></div></div><div class="mobile_data_item is_center">      <div class="mobile_data_link"><a href="/tags/"><div class="headline">Tags</div><div class="length_num">67</div></a></div></div></div><hr><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fa fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fa fa-archive"></i><span> 档案</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fa fa-folder-open"></i><span> 类别</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fa fa-link"></i><span> 链接</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fa fa-heart"></i><span> 关于</span></a></div><div class="menus_item"><a class="site-page"><i class="fa-fw fa fa-list" aria-hidden="true"></i><span> 学习和娱乐</span><i class="fa fa-chevron-down menus-expand" aria-hidden="true"></i></a><ul class="menus_item_child"><li><a class="site-page" href="/music/"><i class="fa-fw fa fa-music"></i><span> 音乐</span></a></li><li><a class="site-page" href="/movies/"><i class="fa-fw fa fa-film"></i><span> 电影</span></a></li><li><a class="site-page" href="/book/"><i class="fa-fw fa fa-book"></i><span> 书籍</span></a></li></ul></div></div></div><div id="mobile-sidebar-toc"><div class="toc_mobile_headline">Catalog</div><ol class="toc_mobile_items"><li class="toc_mobile_items-item toc_mobile_items-level-1"><a class="toc_mobile_items-link" href="#run-py"><span class="toc_mobile_items-number">1.</span> <span class="toc_mobile_items-text">run.py</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-1"><a class="toc_mobile_items-link" href="#train-eval-py"><span class="toc_mobile_items-number">2.</span> <span class="toc_mobile_items-text">train_eval.py</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-1"><a class="toc_mobile_items-link" href="#utils-py"><span class="toc_mobile_items-number">3.</span> <span class="toc_mobile_items-text">utils.py</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-1"><a class="toc_mobile_items-link" href="#utils-fasttext-py"><span class="toc_mobile_items-number">4.</span> <span class="toc_mobile_items-text">utils_fasttext.py</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-1"><a class="toc_mobile_items-link" href="#TextRNN-py"><span class="toc_mobile_items-number">5.</span> <span class="toc_mobile_items-text">TextRNN.py</span></a></li></ol></div></div><div id="body-wrap"><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true">     </i><div class="auto_open" id="sidebar"><div class="sidebar-toc"><div class="sidebar-toc__title">Catalog</div><div class="sidebar-toc__progress"><span class="progress-notice">You've read</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar">     </div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#run-py"><span class="toc-number">1.</span> <span class="toc-text">run.py</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#train-eval-py"><span class="toc-number">2.</span> <span class="toc-text">train_eval.py</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#utils-py"><span class="toc-number">3.</span> <span class="toc-text">utils.py</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#utils-fasttext-py"><span class="toc-number">4.</span> <span class="toc-text">utils_fasttext.py</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#TextRNN-py"><span class="toc-number">5.</span> <span class="toc-text">TextRNN.py</span></a></li></ol></div></div></div><div id="content-outer"><div id="top-container" style="background-image: url(https://s1.ax1x.com/2020/03/24/8b2SkF.png)"><div id="post-info"><div id="post-title"><div class="posttitle">RNN网络实战</div></div><div id="post-meta"><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> Created 2020-03-24<span class="post-meta__separator">|</span><i class="fa fa-history" aria-hidden="true"></i> Updated 2020-03-25</time><div class="post-meta-wordcount"><i class="fa fa-eye post-meta__icon" aria-hidden="true">       </i><span>Post View: </span><span id="busuanzi_value_page_pv"></span></div></div></div></div><div class="layout layout_post" id="content-inner">   <article id="post"><div class="article-container" id="post-content"><h1 id="run-py"><a href="#run-py" class="headerlink" title="run.py"></a>run.py</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> train_eval <span class="keyword">import</span> train, init_network</span><br><span class="line"><span class="keyword">from</span> importlib <span class="keyword">import</span> import_module</span><br><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line"><span class="keyword">from</span> tensorboardX <span class="keyword">import</span> SummaryWriter</span><br><span class="line"></span><br><span class="line">parser = argparse.ArgumentParser(description=<span class="string">'Chinese Text Classification'</span>)</span><br><span class="line">parser.add_argument(<span class="string">'--model'</span>, type=str, required=<span class="literal">True</span>, help=<span class="string">'choose a model: TextCNN, TextRNN, FastText, TextRCNN, TextRNN_Att, DPCNN, Transformer'</span>)</span><br><span class="line">parser.add_argument(<span class="string">'--embedding'</span>, default=<span class="string">'pre_trained'</span>, type=str, help=<span class="string">'random or pre_trained'</span>)</span><br><span class="line">parser.add_argument(<span class="string">'--word'</span>, default=<span class="literal">False</span>, type=bool, help=<span class="string">'True for word, False for char'</span>)</span><br><span class="line">args = parser.parse_args()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    dataset = <span class="string">'THUCNews'</span>  <span class="comment"># 数据集</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 搜狗新闻:embedding_SougouNews.npz, 腾讯:embedding_Tencent.npz,用此词嵌入进行训练 在rnn任务中利用现成的即可（对于大众化词汇） 随机初始化:random</span></span><br><span class="line">    embedding = <span class="string">'embedding_SougouNews.npz'</span></span><br><span class="line">    <span class="keyword">if</span> args.embedding == <span class="string">'random'</span>:</span><br><span class="line">        embedding = <span class="string">'random'</span></span><br><span class="line">    model_name = args.model  <span class="comment">#TextCNN, TextRNN,</span></span><br><span class="line">    <span class="keyword">if</span> model_name == <span class="string">'FastText'</span>:</span><br><span class="line">        <span class="keyword">from</span> utils_fasttext <span class="keyword">import</span> build_dataset, build_iterator, get_time_dif</span><br><span class="line">        embedding = <span class="string">'random'</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">from</span> utils <span class="keyword">import</span> build_dataset, build_iterator, get_time_dif</span><br><span class="line"></span><br><span class="line">    x = import_module(<span class="string">'models.'</span> + model_name)</span><br><span class="line">    config = x.Config(dataset, embedding) <span class="comment">#装载embedding和dataset</span></span><br><span class="line">    np.random.seed(<span class="number">1</span>)                                   <span class="comment">#随机种子 保证每次运行的结果都</span></span><br><span class="line">    torch.manual_seed(<span class="number">1</span>)                                <span class="comment">#不变，即为控制变量思想，只看</span></span><br><span class="line">    torch.cuda.manual_seed_all(<span class="number">1</span>)                       <span class="comment">#参数带来的影响</span></span><br><span class="line">    torch.backends.cudnn.deterministic = <span class="literal">True</span>           <span class="comment">#</span></span><br><span class="line"></span><br><span class="line">    start_time = time.time()</span><br><span class="line">    print(<span class="string">"Loading data..."</span>)</span><br><span class="line">    vocab, train_data, dev_data, test_data = build_dataset(config, args.word)</span><br><span class="line">    train_iter = build_iterator(train_data, config)</span><br><span class="line">    dev_iter = build_iterator(dev_data, config)</span><br><span class="line">    test_iter = build_iterator(test_data, config)</span><br><span class="line">    time_dif = get_time_dif(start_time)</span><br><span class="line">    print(<span class="string">"Time usage:"</span>, time_dif)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># train</span></span><br><span class="line">    config.n_vocab = len(vocab)</span><br><span class="line">    model = x.Model(config).to(config.device)</span><br><span class="line">    writer = SummaryWriter(log_dir=config.log_path + <span class="string">'/'</span> + time.strftime(<span class="string">'%m-%d_%H.%M'</span>, time.localtime()))</span><br><span class="line">    <span class="keyword">if</span> model_name != <span class="string">'Transformer'</span>:</span><br><span class="line">        init_network(model)</span><br><span class="line">    print(model.parameters)</span><br><span class="line">    train(config, model, train_iter, dev_iter, test_iter,writer)</span><br></pre></td></tr></table></figure>

<h1 id="train-eval-py"><a href="#train-eval-py" class="headerlink" title="train_eval.py"></a>train_eval.py</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># coding: UTF-8</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> metrics</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">from</span> utils <span class="keyword">import</span> get_time_dif</span><br><span class="line"><span class="keyword">from</span> tensorboardX <span class="keyword">import</span> SummaryWriter</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 权重初始化，默认xavier</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">init_network</span><span class="params">(model, method=<span class="string">'xavier'</span>, exclude=<span class="string">'embedding'</span>, seed=<span class="number">123</span>)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> name, w <span class="keyword">in</span> model.named_parameters():</span><br><span class="line">        <span class="keyword">if</span> exclude <span class="keyword">not</span> <span class="keyword">in</span> name:</span><br><span class="line">            <span class="keyword">if</span> <span class="string">'weight'</span> <span class="keyword">in</span> name:</span><br><span class="line">                <span class="keyword">if</span> method == <span class="string">'xavier'</span>:</span><br><span class="line">                    nn.init.xavier_normal_(w)</span><br><span class="line">                <span class="keyword">elif</span> method == <span class="string">'kaiming'</span>:</span><br><span class="line">                    nn.init.kaiming_normal_(w)</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    nn.init.normal_(w)</span><br><span class="line">            <span class="keyword">elif</span> <span class="string">'bias'</span> <span class="keyword">in</span> name:</span><br><span class="line">                nn.init.constant_(w, <span class="number">0</span>)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(config, model, train_iter, dev_iter, test_iter,writer)</span>:</span></span><br><span class="line">    start_time = time.time()</span><br><span class="line">    model.train()</span><br><span class="line">    optimizer = torch.optim.Adam(model.parameters(), lr=config.learning_rate)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 学习率指数衰减，每次epoch：学习率 = gamma * 学习率</span></span><br><span class="line">    <span class="comment"># scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)</span></span><br><span class="line">    total_batch = <span class="number">0</span>  <span class="comment"># 记录进行到多少batch</span></span><br><span class="line">    dev_best_loss = float(<span class="string">'inf'</span>) <span class="comment">#loss值先设置为无穷大</span></span><br><span class="line">    last_improve = <span class="number">0</span>  <span class="comment"># 记录上次验证集loss下降的batch数</span></span><br><span class="line">    flag = <span class="literal">False</span>  <span class="comment"># 记录是否很久没有效果提升</span></span><br><span class="line">    <span class="comment">#writer = SummaryWriter(log_dir=config.log_path + '/' + time.strftime('%m-%d_%H.%M', time.localtime()))</span></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> range(config.num_epochs):</span><br><span class="line">        print(<span class="string">'Epoch [&#123;&#125;/&#123;&#125;]'</span>.format(epoch + <span class="number">1</span>, config.num_epochs))</span><br><span class="line">        <span class="comment"># scheduler.step() # 学习率衰减</span></span><br><span class="line">        <span class="keyword">for</span> i, (trains, labels) <span class="keyword">in</span> enumerate(train_iter):</span><br><span class="line">            <span class="comment">#print (trains[0].shape)</span></span><br><span class="line">            outputs = model(trains)</span><br><span class="line">            model.zero_grad()</span><br><span class="line">            loss = F.cross_entropy(outputs, labels)</span><br><span class="line">            loss.backward()</span><br><span class="line">            optimizer.step()</span><br><span class="line">            <span class="keyword">if</span> total_batch % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">                <span class="comment"># 每多少轮输出在训练集和验证集上的效果</span></span><br><span class="line">                true = labels.data.cpu()</span><br><span class="line">                predic = torch.max(outputs.data, <span class="number">1</span>)[<span class="number">1</span>].cpu()</span><br><span class="line">                train_acc = metrics.accuracy_score(true, predic)</span><br><span class="line">                dev_acc, dev_loss = evaluate(config, model, dev_iter)</span><br><span class="line">                <span class="keyword">if</span> dev_loss &lt; dev_best_loss:</span><br><span class="line">                    dev_best_loss = dev_loss</span><br><span class="line">                    torch.save(model.state_dict(), config.save_path)    <span class="comment">#验证集合效果好，我们就进行保存</span></span><br><span class="line">                    improve = <span class="string">'*'</span></span><br><span class="line">                    last_improve = total_batch</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    improve = <span class="string">''</span></span><br><span class="line">                time_dif = get_time_dif(start_time)</span><br><span class="line">                msg = <span class="string">'Iter: &#123;0:&gt;6&#125;,  Train Loss: &#123;1:&gt;5.2&#125;,  Train Acc: &#123;2:&gt;6.2%&#125;,  Val Loss: &#123;3:&gt;5.2&#125;,  Val Acc: &#123;4:&gt;6.2%&#125;,  Time: &#123;5&#125; &#123;6&#125;'</span></span><br><span class="line">                print(msg.format(total_batch, loss.item(), train_acc, dev_loss, dev_acc, time_dif, improve))</span><br><span class="line">                writer.add_scalar(<span class="string">"loss/train"</span>, loss.item(), total_batch)</span><br><span class="line">                writer.add_scalar(<span class="string">"loss/dev"</span>, dev_loss, total_batch)</span><br><span class="line">                writer.add_scalar(<span class="string">"acc/train"</span>, train_acc, total_batch)</span><br><span class="line">                writer.add_scalar(<span class="string">"acc/dev"</span>, dev_acc, total_batch)</span><br><span class="line">                model.train()</span><br><span class="line">            total_batch += <span class="number">1</span></span><br><span class="line">            <span class="keyword">if</span> total_batch - last_improve &gt; config.require_improvement:</span><br><span class="line">                <span class="comment"># 验证集loss超过1000batch没下降，结束训练</span></span><br><span class="line">                print(<span class="string">"No optimization for a long time, auto-stopping..."</span>)</span><br><span class="line">                flag = <span class="literal">True</span></span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">        <span class="keyword">if</span> flag:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">    writer.close()</span><br><span class="line">    test(config, model, test_iter)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test</span><span class="params">(config, model, test_iter)</span>:</span></span><br><span class="line">    <span class="comment"># test</span></span><br><span class="line">    model.load_state_dict(torch.load(config.save_path))</span><br><span class="line">    model.eval()</span><br><span class="line">    start_time = time.time()</span><br><span class="line">    test_acc, test_loss, test_report, test_confusion = evaluate(config, model, test_iter, test=<span class="literal">True</span>)</span><br><span class="line">    msg = <span class="string">'Test Loss: &#123;0:&gt;5.2&#125;,  Test Acc: &#123;1:&gt;6.2%&#125;'</span></span><br><span class="line">    print(msg.format(test_loss, test_acc))</span><br><span class="line">    print(<span class="string">"Precision, Recall and F1-Score..."</span>)</span><br><span class="line">    print(test_report)</span><br><span class="line">    print(<span class="string">"Confusion Matrix..."</span>)</span><br><span class="line">    print(test_confusion)</span><br><span class="line">    time_dif = get_time_dif(start_time)</span><br><span class="line">    print(<span class="string">"Time usage:"</span>, time_dif)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">evaluate</span><span class="params">(config, model, data_iter, test=False)</span>:</span></span><br><span class="line">    model.eval()</span><br><span class="line">    loss_total = <span class="number">0</span></span><br><span class="line">    predict_all = np.array([], dtype=int)</span><br><span class="line">    labels_all = np.array([], dtype=int)</span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="keyword">for</span> texts, labels <span class="keyword">in</span> data_iter:</span><br><span class="line">            outputs = model(texts)</span><br><span class="line">            loss = F.cross_entropy(outputs, labels)</span><br><span class="line">            loss_total += loss</span><br><span class="line">            labels = labels.data.cpu().numpy()</span><br><span class="line">            predic = torch.max(outputs.data, <span class="number">1</span>)[<span class="number">1</span>].cpu().numpy()</span><br><span class="line">            labels_all = np.append(labels_all, labels)</span><br><span class="line">            predict_all = np.append(predict_all, predic)</span><br><span class="line"></span><br><span class="line">    acc = metrics.accuracy_score(labels_all, predict_all)</span><br><span class="line">    <span class="keyword">if</span> test:</span><br><span class="line">        report = metrics.classification_report(labels_all, predict_all, target_names=config.class_list, digits=<span class="number">4</span>)</span><br><span class="line">        confusion = metrics.confusion_matrix(labels_all, predict_all)</span><br><span class="line">        <span class="keyword">return</span> acc, loss_total / len(data_iter), report, confusion</span><br><span class="line">    <span class="keyword">return</span> acc, loss_total / len(data_iter)</span><br></pre></td></tr></table></figure>

<h1 id="utils-py"><a href="#utils-py" class="headerlink" title="utils.py"></a>utils.py</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># coding: UTF-8</span></span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pickle <span class="keyword">as</span> pkl</span><br><span class="line"><span class="keyword">from</span> tqdm <span class="keyword">import</span> tqdm</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">from</span> datetime <span class="keyword">import</span> timedelta</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">MAX_VOCAB_SIZE = <span class="number">10000</span>  <span class="comment"># 词表长度限制</span></span><br><span class="line">UNK, PAD = <span class="string">'&lt;UNK&gt;'</span>, <span class="string">'&lt;PAD&gt;'</span>  <span class="comment"># 未知字，padding符号</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">build_vocab</span><span class="params">(file_path, tokenizer, max_size, min_freq)</span>:</span></span><br><span class="line">    vocab_dic = &#123;&#125;</span><br><span class="line">    <span class="keyword">with</span> open(file_path, <span class="string">'r'</span>, encoding=<span class="string">'UTF-8'</span>) <span class="keyword">as</span> f:</span><br><span class="line">        <span class="keyword">for</span> line <span class="keyword">in</span> tqdm(f):</span><br><span class="line">            lin = line.strip()</span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> lin:</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            content = lin.split(<span class="string">'\t'</span>)[<span class="number">0</span>]</span><br><span class="line">            <span class="keyword">for</span> word <span class="keyword">in</span> tokenizer(content):</span><br><span class="line">                vocab_dic[word] = vocab_dic.get(word, <span class="number">0</span>) + <span class="number">1</span></span><br><span class="line">        vocab_list = sorted([_ <span class="keyword">for</span> _ <span class="keyword">in</span> vocab_dic.items() <span class="keyword">if</span> _[<span class="number">1</span>] &gt;= min_freq], key=<span class="keyword">lambda</span> x: x[<span class="number">1</span>], reverse=<span class="literal">True</span>)[:max_size]</span><br><span class="line">        vocab_dic = &#123;word_count[<span class="number">0</span>]: idx <span class="keyword">for</span> idx, word_count <span class="keyword">in</span> enumerate(vocab_list)&#125;</span><br><span class="line">        vocab_dic.update(&#123;UNK: len(vocab_dic), PAD: len(vocab_dic) + <span class="number">1</span>&#125;)</span><br><span class="line">    <span class="keyword">return</span> vocab_dic</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">build_dataset</span><span class="params">(config, ues_word)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> ues_word:</span><br><span class="line">        tokenizer = <span class="keyword">lambda</span> x: x.split(<span class="string">' '</span>)  <span class="comment"># 以空格隔开，word-level</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        tokenizer = <span class="keyword">lambda</span> x: [y <span class="keyword">for</span> y <span class="keyword">in</span> x]  <span class="comment"># char-level</span></span><br><span class="line">    <span class="keyword">if</span> os.path.exists(config.vocab_path):</span><br><span class="line">        vocab = pkl.load(open(config.vocab_path, <span class="string">'rb'</span>))</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        vocab = build_vocab(config.train_path, tokenizer=tokenizer, max_size=MAX_VOCAB_SIZE, min_freq=<span class="number">1</span>)</span><br><span class="line">        pkl.dump(vocab, open(config.vocab_path, <span class="string">'wb'</span>))</span><br><span class="line">    print(<span class="string">f"Vocab size: <span class="subst">&#123;len(vocab)&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">load_dataset</span><span class="params">(path, pad_size=<span class="number">32</span>)</span>:</span></span><br><span class="line">        contents = []</span><br><span class="line">        <span class="keyword">with</span> open(path, <span class="string">'r'</span>, encoding=<span class="string">'UTF-8'</span>) <span class="keyword">as</span> f:</span><br><span class="line">            <span class="keyword">for</span> line <span class="keyword">in</span> tqdm(f):</span><br><span class="line">                lin = line.strip()</span><br><span class="line">                <span class="keyword">if</span> <span class="keyword">not</span> lin:</span><br><span class="line">                    <span class="keyword">continue</span></span><br><span class="line">                content, label = lin.split(<span class="string">'\t'</span>)    <span class="comment">#将字符串和标签分割</span></span><br><span class="line">                words_line = []                     <span class="comment">#声明空列表</span></span><br><span class="line">                token = tokenizer(content)          <span class="comment">#将字符串分割</span></span><br><span class="line">                seq_len = len(token)</span><br><span class="line">                <span class="keyword">if</span> pad_size:</span><br><span class="line">                    <span class="keyword">if</span> len(token) &lt; pad_size:</span><br><span class="line">                        token.extend([vocab.get(PAD)] * (pad_size - len(token)))   <span class="comment">#不够则补充元素</span></span><br><span class="line">                    <span class="keyword">else</span>:</span><br><span class="line">                        token = token[:pad_size]                                    <span class="comment">#够了则截断</span></span><br><span class="line">                        seq_len = pad_size                                          <span class="comment">#更新大小为pad_size</span></span><br><span class="line">                <span class="comment"># word to id</span></span><br><span class="line">                <span class="keyword">for</span> word <span class="keyword">in</span> token:</span><br><span class="line">                    words_line.append(vocab.get(word, vocab.get(UNK))) <span class="comment">#将词语转换为索引</span></span><br><span class="line">                contents.append((words_line, int(label), seq_len))</span><br><span class="line">        <span class="keyword">return</span> contents  <span class="comment"># [([...], 0), ([...], 1), ...]</span></span><br><span class="line">    train = load_dataset(config.train_path, config.pad_size)</span><br><span class="line">    dev = load_dataset(config.dev_path, config.pad_size)</span><br><span class="line">    test = load_dataset(config.test_path, config.pad_size)</span><br><span class="line">    <span class="keyword">return</span> vocab, train, dev, test</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DatasetIterater</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, batches, batch_size, device)</span>:</span></span><br><span class="line">        self.batch_size = batch_size</span><br><span class="line">        self.batches = batches</span><br><span class="line">        self.n_batches = len(batches) // batch_size</span><br><span class="line">        self.residue = <span class="literal">False</span>  <span class="comment"># 记录batch数量是否为整数</span></span><br><span class="line">        <span class="keyword">if</span> len(batches) % self.n_batches != <span class="number">0</span>:</span><br><span class="line">            self.residue = <span class="literal">True</span></span><br><span class="line">        self.index = <span class="number">0</span></span><br><span class="line">        self.device = device</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_to_tensor</span><span class="params">(self, datas)</span>:</span></span><br><span class="line">        x = torch.LongTensor([_[<span class="number">0</span>] <span class="keyword">for</span> _ <span class="keyword">in</span> datas]).to(self.device)</span><br><span class="line">        y = torch.LongTensor([_[<span class="number">1</span>] <span class="keyword">for</span> _ <span class="keyword">in</span> datas]).to(self.device)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># pad前的长度(超过pad_size的设为pad_size)</span></span><br><span class="line">        seq_len = torch.LongTensor([_[<span class="number">2</span>] <span class="keyword">for</span> _ <span class="keyword">in</span> datas]).to(self.device)</span><br><span class="line">        <span class="keyword">return</span> (x, seq_len), y</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__next__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> self.residue <span class="keyword">and</span> self.index == self.n_batches:</span><br><span class="line">            batches = self.batches[self.index * self.batch_size: len(self.batches)]</span><br><span class="line">            self.index += <span class="number">1</span></span><br><span class="line">            batches = self._to_tensor(batches)</span><br><span class="line">            <span class="keyword">return</span> batches</span><br><span class="line"></span><br><span class="line">        <span class="keyword">elif</span> self.index &gt; self.n_batches:</span><br><span class="line">            self.index = <span class="number">0</span></span><br><span class="line">            <span class="keyword">raise</span> StopIteration</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            batches = self.batches[self.index * self.batch_size: (self.index + <span class="number">1</span>) * self.batch_size]</span><br><span class="line">            self.index += <span class="number">1</span></span><br><span class="line">            batches = self._to_tensor(batches)</span><br><span class="line">            <span class="keyword">return</span> batches</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__iter__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> self.residue:</span><br><span class="line">            <span class="keyword">return</span> self.n_batches + <span class="number">1</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> self.n_batches</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">build_iterator</span><span class="params">(dataset, config)</span>:</span></span><br><span class="line">    iter = DatasetIterater(dataset, config.batch_size, config.device)</span><br><span class="line">    <span class="keyword">return</span> iter</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_time_dif</span><span class="params">(start_time)</span>:</span></span><br><span class="line">    <span class="string">"""获取已使用时间"""</span></span><br><span class="line">    end_time = time.time()</span><br><span class="line">    time_dif = end_time - start_time</span><br><span class="line">    <span class="keyword">return</span> timedelta(seconds=int(round(time_dif)))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    <span class="string">'''提取预训练词向量'''</span></span><br><span class="line">    <span class="comment"># 下面的目录、文件名按需更改。</span></span><br><span class="line">    train_dir = <span class="string">"./THUCNews/data/train.txt"</span></span><br><span class="line">    vocab_dir = <span class="string">"./THUCNews/data/vocab.pkl"</span></span><br><span class="line">    pretrain_dir = <span class="string">"./THUCNews/data/sgns.sogou.char"</span></span><br><span class="line">    emb_dim = <span class="number">300</span></span><br><span class="line">    filename_trimmed_dir = <span class="string">"./THUCNews/data/embedding_SougouNews"</span></span><br><span class="line">    <span class="keyword">if</span> os.path.exists(vocab_dir):</span><br><span class="line">        word_to_id = pkl.load(open(vocab_dir, <span class="string">'rb'</span>))</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="comment"># tokenizer = lambda x: x.split(' ')  # 以词为单位构建词表(数据集中词之间以空格隔开)</span></span><br><span class="line">        tokenizer = <span class="keyword">lambda</span> x: [y <span class="keyword">for</span> y <span class="keyword">in</span> x]  <span class="comment"># 以字为单位构建词表</span></span><br><span class="line">        word_to_id = build_vocab(train_dir, tokenizer=tokenizer, max_size=MAX_VOCAB_SIZE, min_freq=<span class="number">1</span>)</span><br><span class="line">        pkl.dump(word_to_id, open(vocab_dir, <span class="string">'wb'</span>))</span><br><span class="line"></span><br><span class="line">    embeddings = np.random.rand(len(word_to_id), emb_dim)</span><br><span class="line">    f = open(pretrain_dir, <span class="string">"r"</span>, encoding=<span class="string">'UTF-8'</span>)</span><br><span class="line">    <span class="keyword">for</span> i, line <span class="keyword">in</span> enumerate(f.readlines()):</span><br><span class="line">        <span class="comment"># if i == 0:  # 若第一行是标题，则跳过</span></span><br><span class="line">        <span class="comment">#     continue</span></span><br><span class="line">        lin = line.strip().split(<span class="string">" "</span>)</span><br><span class="line">        <span class="keyword">if</span> lin[<span class="number">0</span>] <span class="keyword">in</span> word_to_id:</span><br><span class="line">            idx = word_to_id[lin[<span class="number">0</span>]]</span><br><span class="line">            emb = [float(x) <span class="keyword">for</span> x <span class="keyword">in</span> lin[<span class="number">1</span>:<span class="number">301</span>]]</span><br><span class="line">            embeddings[idx] = np.asarray(emb, dtype=<span class="string">'float32'</span>)</span><br><span class="line">    f.close()</span><br><span class="line">    np.savez_compressed(filename_trimmed_dir, embeddings=embeddings)</span><br></pre></td></tr></table></figure>

<h1 id="utils-fasttext-py"><a href="#utils-fasttext-py" class="headerlink" title="utils_fasttext.py"></a>utils_fasttext.py</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># coding: UTF-8</span></span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pickle <span class="keyword">as</span> pkl</span><br><span class="line"><span class="keyword">from</span> tqdm <span class="keyword">import</span> tqdm</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">from</span> datetime <span class="keyword">import</span> timedelta</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">MAX_VOCAB_SIZE = <span class="number">10000</span>  <span class="comment"># 词表长度限制</span></span><br><span class="line">UNK, PAD = <span class="string">'&lt;UNK&gt;'</span>, <span class="string">'&lt;PAD&gt;'</span>  <span class="comment"># 未知字，padding符号</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">build_vocab</span><span class="params">(file_path, tokenizer, max_size, min_freq)</span>:</span></span><br><span class="line">    vocab_dic = &#123;&#125;</span><br><span class="line">    <span class="keyword">with</span> open(file_path, <span class="string">'r'</span>, encoding=<span class="string">'UTF-8'</span>) <span class="keyword">as</span> f:</span><br><span class="line">        <span class="keyword">for</span> line <span class="keyword">in</span> tqdm(f):</span><br><span class="line">            lin = line.strip()</span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> lin:</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            content = lin.split(<span class="string">'\t'</span>)[<span class="number">0</span>]</span><br><span class="line">            <span class="keyword">for</span> word <span class="keyword">in</span> tokenizer(content):</span><br><span class="line">                vocab_dic[word] = vocab_dic.get(word, <span class="number">0</span>) + <span class="number">1</span></span><br><span class="line">        vocab_list = sorted([_ <span class="keyword">for</span> _ <span class="keyword">in</span> vocab_dic.items() <span class="keyword">if</span> _[<span class="number">1</span>] &gt;= min_freq], key=<span class="keyword">lambda</span> x: x[<span class="number">1</span>], reverse=<span class="literal">True</span>)[:max_size]</span><br><span class="line">        vocab_dic = &#123;word_count[<span class="number">0</span>]: idx <span class="keyword">for</span> idx, word_count <span class="keyword">in</span> enumerate(vocab_list)&#125;</span><br><span class="line">        vocab_dic.update(&#123;UNK: len(vocab_dic), PAD: len(vocab_dic) + <span class="number">1</span>&#125;)</span><br><span class="line">    <span class="keyword">return</span> vocab_dic</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">build_dataset</span><span class="params">(config, ues_word)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> ues_word:</span><br><span class="line">        tokenizer = <span class="keyword">lambda</span> x: x.split(<span class="string">' '</span>)  <span class="comment"># 以空格隔开，word-level</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        tokenizer = <span class="keyword">lambda</span> x: [y <span class="keyword">for</span> y <span class="keyword">in</span> x]  <span class="comment"># char-level</span></span><br><span class="line">    <span class="keyword">if</span> os.path.exists(config.vocab_path):</span><br><span class="line">        vocab = pkl.load(open(config.vocab_path, <span class="string">'rb'</span>))</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        vocab = build_vocab(config.train_path, tokenizer=tokenizer, max_size=MAX_VOCAB_SIZE, min_freq=<span class="number">1</span>)</span><br><span class="line">        pkl.dump(vocab, open(config.vocab_path, <span class="string">'wb'</span>))</span><br><span class="line">    print(<span class="string">f"Vocab size: <span class="subst">&#123;len(vocab)&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">load_dataset</span><span class="params">(path, pad_size=<span class="number">32</span>)</span>:</span></span><br><span class="line">        contents = []</span><br><span class="line">        <span class="keyword">with</span> open(path, <span class="string">'r'</span>, encoding=<span class="string">'UTF-8'</span>) <span class="keyword">as</span> f:</span><br><span class="line">            <span class="keyword">for</span> line <span class="keyword">in</span> tqdm(f):</span><br><span class="line">                lin = line.strip()</span><br><span class="line">                <span class="keyword">if</span> <span class="keyword">not</span> lin:</span><br><span class="line">                    <span class="keyword">continue</span></span><br><span class="line">                content, label = lin.split(<span class="string">'\t'</span>)    <span class="comment">#将字符串和标签分割</span></span><br><span class="line">                words_line = []                     <span class="comment">#声明空列表</span></span><br><span class="line">                token = tokenizer(content)          <span class="comment">#将字符串分割</span></span><br><span class="line">                seq_len = len(token)</span><br><span class="line">                <span class="keyword">if</span> pad_size:</span><br><span class="line">                    <span class="keyword">if</span> len(token) &lt; pad_size:</span><br><span class="line">                        token.extend([vocab.get(PAD)] * (pad_size - len(token)))   <span class="comment">#不够则补充元素</span></span><br><span class="line">                    <span class="keyword">else</span>:</span><br><span class="line">                        token = token[:pad_size]                                    <span class="comment">#够了则截断</span></span><br><span class="line">                        seq_len = pad_size                                          <span class="comment">#更新大小为pad_size</span></span><br><span class="line">                <span class="comment"># word to id</span></span><br><span class="line">                <span class="keyword">for</span> word <span class="keyword">in</span> token:</span><br><span class="line">                    words_line.append(vocab.get(word, vocab.get(UNK))) <span class="comment">#将词语转换为索引</span></span><br><span class="line">                contents.append((words_line, int(label), seq_len))</span><br><span class="line">        <span class="keyword">return</span> contents  <span class="comment"># [([...], 0), ([...], 1), ...]</span></span><br><span class="line">    train = load_dataset(config.train_path, config.pad_size)</span><br><span class="line">    dev = load_dataset(config.dev_path, config.pad_size)</span><br><span class="line">    test = load_dataset(config.test_path, config.pad_size)</span><br><span class="line">    <span class="keyword">return</span> vocab, train, dev, test</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DatasetIterater</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, batches, batch_size, device)</span>:</span></span><br><span class="line">        self.batch_size = batch_size</span><br><span class="line">        self.batches = batches</span><br><span class="line">        self.n_batches = len(batches) // batch_size</span><br><span class="line">        self.residue = <span class="literal">False</span>  <span class="comment"># 记录batch数量是否为整数</span></span><br><span class="line">        <span class="keyword">if</span> len(batches) % self.n_batches != <span class="number">0</span>:</span><br><span class="line">            self.residue = <span class="literal">True</span></span><br><span class="line">        self.index = <span class="number">0</span></span><br><span class="line">        self.device = device</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_to_tensor</span><span class="params">(self, datas)</span>:</span></span><br><span class="line">        x = torch.LongTensor([_[<span class="number">0</span>] <span class="keyword">for</span> _ <span class="keyword">in</span> datas]).to(self.device)</span><br><span class="line">        y = torch.LongTensor([_[<span class="number">1</span>] <span class="keyword">for</span> _ <span class="keyword">in</span> datas]).to(self.device)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># pad前的长度(超过pad_size的设为pad_size)</span></span><br><span class="line">        seq_len = torch.LongTensor([_[<span class="number">2</span>] <span class="keyword">for</span> _ <span class="keyword">in</span> datas]).to(self.device)</span><br><span class="line">        <span class="keyword">return</span> (x, seq_len), y</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__next__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> self.residue <span class="keyword">and</span> self.index == self.n_batches:</span><br><span class="line">            batches = self.batches[self.index * self.batch_size: len(self.batches)]</span><br><span class="line">            self.index += <span class="number">1</span></span><br><span class="line">            batches = self._to_tensor(batches)</span><br><span class="line">            <span class="keyword">return</span> batches</span><br><span class="line"></span><br><span class="line">        <span class="keyword">elif</span> self.index &gt; self.n_batches:</span><br><span class="line">            self.index = <span class="number">0</span></span><br><span class="line">            <span class="keyword">raise</span> StopIteration</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            batches = self.batches[self.index * self.batch_size: (self.index + <span class="number">1</span>) * self.batch_size]</span><br><span class="line">            self.index += <span class="number">1</span></span><br><span class="line">            batches = self._to_tensor(batches)</span><br><span class="line">            <span class="keyword">return</span> batches</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__iter__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> self.residue:</span><br><span class="line">            <span class="keyword">return</span> self.n_batches + <span class="number">1</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> self.n_batches</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">build_iterator</span><span class="params">(dataset, config)</span>:</span></span><br><span class="line">    iter = DatasetIterater(dataset, config.batch_size, config.device)</span><br><span class="line">    <span class="keyword">return</span> iter</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_time_dif</span><span class="params">(start_time)</span>:</span></span><br><span class="line">    <span class="string">"""获取已使用时间"""</span></span><br><span class="line">    end_time = time.time()</span><br><span class="line">    time_dif = end_time - start_time</span><br><span class="line">    <span class="keyword">return</span> timedelta(seconds=int(round(time_dif)))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    <span class="string">'''提取预训练词向量'''</span></span><br><span class="line">    <span class="comment"># 下面的目录、文件名按需更改。</span></span><br><span class="line">    train_dir = <span class="string">"./THUCNews/data/train.txt"</span></span><br><span class="line">    vocab_dir = <span class="string">"./THUCNews/data/vocab.pkl"</span></span><br><span class="line">    pretrain_dir = <span class="string">"./THUCNews/data/sgns.sogou.char"</span></span><br><span class="line">    emb_dim = <span class="number">300</span></span><br><span class="line">    filename_trimmed_dir = <span class="string">"./THUCNews/data/embedding_SougouNews"</span></span><br><span class="line">    <span class="keyword">if</span> os.path.exists(vocab_dir):</span><br><span class="line">        word_to_id = pkl.load(open(vocab_dir, <span class="string">'rb'</span>))</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="comment"># tokenizer = lambda x: x.split(' ')  # 以词为单位构建词表(数据集中词之间以空格隔开)</span></span><br><span class="line">        tokenizer = <span class="keyword">lambda</span> x: [y <span class="keyword">for</span> y <span class="keyword">in</span> x]  <span class="comment"># 以字为单位构建词表</span></span><br><span class="line">        word_to_id = build_vocab(train_dir, tokenizer=tokenizer, max_size=MAX_VOCAB_SIZE, min_freq=<span class="number">1</span>)</span><br><span class="line">        pkl.dump(word_to_id, open(vocab_dir, <span class="string">'wb'</span>))</span><br><span class="line"></span><br><span class="line">    embeddings = np.random.rand(len(word_to_id), emb_dim)</span><br><span class="line">    f = open(pretrain_dir, <span class="string">"r"</span>, encoding=<span class="string">'UTF-8'</span>)</span><br><span class="line">    <span class="keyword">for</span> i, line <span class="keyword">in</span> enumerate(f.readlines()):</span><br><span class="line">        <span class="comment"># if i == 0:  # 若第一行是标题，则跳过</span></span><br><span class="line">        <span class="comment">#     continue</span></span><br><span class="line">        lin = line.strip().split(<span class="string">" "</span>)</span><br><span class="line">        <span class="keyword">if</span> lin[<span class="number">0</span>] <span class="keyword">in</span> word_to_id:</span><br><span class="line">            idx = word_to_id[lin[<span class="number">0</span>]]</span><br><span class="line">            emb = [float(x) <span class="keyword">for</span> x <span class="keyword">in</span> lin[<span class="number">1</span>:<span class="number">301</span>]]</span><br><span class="line">            embeddings[idx] = np.asarray(emb, dtype=<span class="string">'float32'</span>)</span><br><span class="line">    f.close()</span><br><span class="line">    np.savez_compressed(filename_trimmed_dir, embeddings=embeddings)</span><br></pre></td></tr></table></figure>

<h1 id="TextRNN-py"><a href="#TextRNN-py" class="headerlink" title="TextRNN.py"></a>TextRNN.py</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># coding: UTF-8</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Config</span><span class="params">(object)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="string">"""配置参数"""</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, dataset, embedding)</span>:</span></span><br><span class="line">        self.model_name = <span class="string">'TextRNN'</span></span><br><span class="line">        self.train_path = dataset + <span class="string">'/data/train.txt'</span>                                <span class="comment"># 训练集</span></span><br><span class="line">        self.dev_path = dataset + <span class="string">'/data/dev.txt'</span>                                    <span class="comment"># 验证集</span></span><br><span class="line">        self.test_path = dataset + <span class="string">'/data/test.txt'</span>                                  <span class="comment"># 测试集</span></span><br><span class="line">        self.class_list = [x.strip() <span class="keyword">for</span> x <span class="keyword">in</span> open(</span><br><span class="line">            dataset + <span class="string">'/data/class.txt'</span>).readlines()]                                <span class="comment"># 类别名单</span></span><br><span class="line">        self.vocab_path = dataset + <span class="string">'/data/vocab.pkl'</span>                                <span class="comment"># 词表（字典结构（索引：词语））</span></span><br><span class="line">        self.save_path = dataset + <span class="string">'/saved_dict/'</span> + self.model_name + <span class="string">'.ckpt'</span>        <span class="comment"># 模型训练结果</span></span><br><span class="line">        self.log_path = dataset + <span class="string">'/log/'</span> + self.model_name</span><br><span class="line">        self.embedding_pretrained = torch.tensor(</span><br><span class="line">            np.load(dataset + <span class="string">'/data/'</span> + embedding)[<span class="string">"embeddings"</span>].astype(<span class="string">'float32'</span>))\</span><br><span class="line">            <span class="keyword">if</span> embedding != <span class="string">'random'</span> <span class="keyword">else</span> <span class="literal">None</span>                                       <span class="comment"># 预训练词向量 #因为是npz格式，所以需要np.load</span></span><br><span class="line">        self.device = torch.device(<span class="string">'cuda'</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">'cpu'</span>)   <span class="comment"># 设备</span></span><br><span class="line"></span><br><span class="line">        self.dropout = <span class="number">0.5</span>                                              <span class="comment"># 随机失活</span></span><br><span class="line">        self.require_improvement = <span class="number">1000</span>                                 <span class="comment"># 若超过1000batch效果还没提升，则提前结束训练</span></span><br><span class="line">        self.num_classes = len(self.class_list)                         <span class="comment"># 类别数</span></span><br><span class="line">        self.n_vocab = <span class="number">0</span>                                                <span class="comment"># 词表大小，在运行时赋值</span></span><br><span class="line">        self.num_epochs = <span class="number">10</span>                                            <span class="comment"># epoch数</span></span><br><span class="line">        self.batch_size = <span class="number">128</span>                                           <span class="comment"># mini-batch大小</span></span><br><span class="line">        self.pad_size = <span class="number">32</span>                                              <span class="comment"># 每句话处理成的长度(短填长切)</span></span><br><span class="line">        self.learning_rate = <span class="number">1e-3</span>                                       <span class="comment"># 学习率</span></span><br><span class="line">        self.embed = self.embedding_pretrained.size(<span class="number">1</span>)\</span><br><span class="line">            <span class="keyword">if</span> self.embedding_pretrained <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">else</span> <span class="number">300</span>           <span class="comment"># 字向量维度, 若使用了预训练词向量，则维度统一</span></span><br><span class="line">        self.hidden_size = <span class="number">128</span>                                          <span class="comment"># lstm隐藏层</span></span><br><span class="line">        self.num_layers = <span class="number">2</span>                                             <span class="comment"># lstm层数</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="string">'''Recurrent Neural Network for Text Classification with Multi-Task Learning'''</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Model</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, config)</span>:</span></span><br><span class="line">        super(Model, self).__init__()</span><br><span class="line">        <span class="keyword">if</span> config.embedding_pretrained <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            self.embedding = nn.Embedding.from_pretrained(config.embedding_pretrained, freeze=<span class="literal">False</span>)  <span class="comment">#embedding层将词汇转化为向量</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.embedding = nn.Embedding(config.n_vocab, config.embed, padding_idx=config.n_vocab - <span class="number">1</span>)</span><br><span class="line">        self.lstm = nn.LSTM(config.embed, config.hidden_size, config.num_layers,                    <span class="comment">#词向量维度  隐层神经元 两层网络</span></span><br><span class="line">                            bidirectional=<span class="literal">True</span>, batch_first=<span class="literal">True</span>, dropout=config.dropout)           <span class="comment">#双向传播  第一维当成batch drop设置成0.5</span></span><br><span class="line">        self.fc = nn.Linear(config.hidden_size * <span class="number">2</span>, config.num_classes)                             <span class="comment">#因为是双向传播（BiLSTM) 所以要乘以2</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        x, _ = x</span><br><span class="line">        out = self.embedding(x)  <span class="comment"># [batch_size, seq_len, embeding]=[128, 32, 300]</span></span><br><span class="line">        out, _ = self.lstm(out)</span><br><span class="line">        out = self.fc(out[:, <span class="number">-1</span>, :])  <span class="comment"># 句子最后时刻的 hidden state  （最后时刻的ht）</span></span><br><span class="line">        <span class="keyword">return</span> out</span><br></pre></td></tr></table></figure>

<p>​     需要在pycharm中配置configurations –model=TextRNN运行。</p>
<p>​    TensorboardX需要本地有tensorflow的支持，运行命令为<code>tensorboard --logdir= 文件夹名</code></p>
<p>​    下附运行结果：</p>
<p>​    <img alt data-src="https://s1.ax1x.com/2020/03/24/8b2SkF.png" class="lazyload"></p>
<p>​                                                                                       <strong>各项指标</strong></p>
<p><img alt data-src="https://s1.ax1x.com/2020/03/24/8bgOO0.png" class="lazyload"></p>
<p>​                                                                                <strong>迷惑矩阵</strong></p>
<p><img alt data-src="https://s1.ax1x.com/2020/03/24/8bgvwT.png" class="lazyload"></p>
<p>​                                                                    <strong>tensorboard将train和dev可视化</strong></p>
<p><img alt data-src="https://s1.ax1x.com/2020/03/24/8b2ilR.png" class="lazyload"></p>
<p>​                                                                              <strong>BiLSTM原理</strong></p>
</div></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">Author: </span><span class="post-copyright-info"><a href="mailto:undefined">superzhaoyang</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">Link: </span><span class="post-copyright-info"><a href="http://yoursite.com/2020/03/24/RNN网络实战/">http://yoursite.com/2020/03/24/RNN网络实战/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">Copyright Notice: </span><span class="post-copyright-info">All articles in this blog are licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> unless stating additionally.</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/循环神经网络/">循环神经网络    </a><a class="post-meta__tags" href="/tags/RNN/">RNN    </a><a class="post-meta__tags" href="/tags/pytorch/">pytorch    </a></div><div class="post_share"><div class="social-share" data-image="https://s1.ax1x.com/2020/03/24/8b2SkF.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js@1.0.16/dist/css/share.min.css"><script src="https://cdn.jsdelivr.net/npm/social-share.js@1.0.16/dist/js/social-share.min.js"></script></div></div><div class="post-reward"><a class="reward-button"><i class="fa fa-qrcode"></i> Donate<div class="reward-main"><ul class="reward-all"><li class="reward-item"><img class="lazyload post-qr-code__img" src="/img/wechat.jpg"><div class="post-qr-code__desc">微信</div></li><li class="reward-item"><img class="lazyload post-qr-code__img" src="/img/alipay.jpg"><div class="post-qr-code__desc">支付宝</div></li></ul></div></a></div><nav class="pagination_post" id="pagination"><div class="prev-post pull_left"><a href="/2020/03/25/数据结构课设实验五之山威地图/"><img class="prev_cover lazyload" data-src="https://s1.ax1x.com/2020/03/25/8v4uTJ.png" onerror="onerror=null;src='/img/404.jpg'"><div class="label">Previous Post</div><div class="prev_info"><span>数据结构课设实验五s之山威地图</span></div></a></div><div class="next-post pull_right"><a href="/2020/03/24/typora快捷键/"><img class="next_cover lazyload" data-src="https://s2.ax1x.com/2019/11/11/MlCiss.jpg" onerror="onerror=null;src='/img/404.jpg'"><div class="label">Next Post</div><div class="next_info"><span>Typora快捷键</span></div></a></div></nav><div class="relatedPosts"><div class="relatedPosts_headline"><i class="fa fa-fw fa-thumbs-up" aria-hidden="true"></i><span> Recommend</span></div><div class="relatedPosts_list"><div class="relatedPosts_item"><a href="/2020/03/22/循环神经网络/" title="循环神经网络RNN"><img class="relatedPosts_cover lazyload" data-src="https://s1.ax1x.com/2020/03/22/84fiEd.png"><div class="relatedPosts_title">循环神经网络RNN</div></a></div></div><div class="clear_both"></div></div><hr><div id="post-comment"><div class="comment_headling"><i class="fa fa-comments fa-fw" aria-hidden="true"></i><span> Comment</span></div><div class="vcomment" id="vcomment"></div><script src="https://cdn.jsdelivr.net/npm/valine/dist/Valine.min.js"></script><script>var notify = false == true ? true : false;
var verify = false == true ? true : false;
var GUEST_INFO = ['nick','mail','link'];
var guest_info = 'nick,mail,link'.split(',').filter(function(item){
  return GUEST_INFO.indexOf(item) > -1
});
guest_info = guest_info.length == 0 ? GUEST_INFO :guest_info;

window.valine = new Valine({
  el:'#vcomment',
  notify:notify,
  verify:verify,
  appId:'REItCuJxdP5eRCIkGnFNRGWt-gzGzoHsz',
  appKey:'CdrBY5eDdonSzQutnKqm0fBv',
  placeholder:'Please leave your footprints',
  avatar:'monsterid',
  guest_info:guest_info,
  pageSize:'10',
  lang:'en',
  recordIP: true
});</script></div></div></div><footer id="footer" style="background-image: url(https://s1.ax1x.com/2020/03/24/8b2SkF.png)"><div id="footer-wrap"><div class="copyright">&copy;2019 - 2020 By superzhaoyang</div><div class="framework-info"><span>Driven </span><a href="http://hexo.io"><span>Hexo</span></a><span class="footer-separator">|</span><span>Theme </span><a href="https://github.com/jerryc127/hexo-theme-butterfly"><span>Butterfly</span></a></div><div class="footer_custom_text">Hi,  welcome  to  my  <a href="https://superzhaoyang.top/">blog</a>!</div></div></footer></div><section class="rightside" id="rightside"><div id="rightside-config-hide"><i class="fa fa-book" id="readmode" title="Read Mode"></i><i class="fa fa-plus" id="font_plus" title="Increase font size"></i><i class="fa fa-minus" id="font_minus" title="Decrease font size"></i><a class="translate_chn_to_cht" id="translateLink" href="javascript:translatePage();" title="Traditional Chinese and Simplified Chinese Conversion" target="_self">繁</a><i class="darkmode fa fa-moon-o" id="darkmode" title="Dark Mode"></i></div><div id="rightside-config-show"><div id="rightside_config" title="Setting"><i class="fa fa-cog" aria-hidden="true"></i></div><a id="to_comment" href="#post-comment" title="Scroll to comment"><i class="scroll_to_comment fa fa-comments">  </i></a><i class="fa fa-list-ul close" id="mobile-toc-button" title="Table of Contents" aria-hidden="true"></i><i class="fa fa-arrow-up" id="go-up" title="Back to top" aria-hidden="true"></i></div></section><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/medium-zoom/dist/medium-zoom.min.js"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
    processEscapes: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
  },
  CommonHTML: {
    linebreaks: { automatic: true, width: "90% container" }
  },
  "HTML-CSS": { 
    linebreaks: { automatic: true, width: "90% container" }
  },
  "SVG": { 
    linebreaks: { automatic: true, width: "90% container" }
  }
});
</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
  var all = MathJax.Hub.getAllJax(), i;
  for (i=0; i < all.length; i += 1) {
    all[i].SourceElement().parentNode.className += ' has-jax';
  }
});
</script><script src="https://cdn.jsdelivr.net/npm/mathjax/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script async src="/js/search/local-search.js"></script><script src="https://cdn.jsdelivr.net/npm/animejs@latest/anime.min.js"></script><script src="/js/third-party/fireworks.js"></script><script src="https://cdn.jsdelivr.net/npm/node-snackbar@0.1.15/dist/snackbar.min.js"></script><script id="ribbon" src="/js/third-party/canvas-ribbon.js" size="150" alpha="0.6" zindex="-1" mobile="true" data-click="true"></script><script id="ribbon_piao" mobile="true" src="/js/third-party/piao.js"></script><script id="canvas_nest" color="0,0,255" opacity="0.7" zindex="-1" count="99" mobile="false" src="/js/third-party/canvas-nest.js"></script><script src="/js/baidupush.js"> </script><script src="/js/third-party/activate-power-mode.js"></script><script>POWERMODE.colorful = true; // make power mode colorful
POWERMODE.shake = true; // turn off shake
document.body.addEventListener('input', POWERMODE);
</script><script src="/js/tw_cn.js"></script><script>translateInitilization()
</script><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><script src="https://cdn.jsdelivr.net/npm/instant.page@latest/instantpage.min.js" type="module"></script><script src="https://cdn.jsdelivr.net/npm/lazysizes@latest/lazysizes.min.js" async></script><script src="/js/third-party/click_heart.js"></script><script src="/js/third-party/ClickShowText.js"></script><div class="search-dialog" id="local-search"><div class="search-dialog__title" id="local-search-title">Local search</div><div id="local-input-panel"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="Search for Posts"></div></div></div><hr><div id="local-search-results"><div id="local-hits"></div><div id="local-stats"><div class="local-search-stats__hr" id="hr"><span>Powered by</span> <a href="https://github.com/wzpan/hexo-generator-search" style="color:#49B1F5;">hexo-generator-search</a></div></div></div><span class="search-close-button"><i class="fa fa-times"></i></span></div><div class="search-mask"></div></body></html>