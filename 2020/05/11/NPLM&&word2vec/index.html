<!DOCTYPE html><html lang="zh-Hans" data-theme="light"><head><meta name="generator" content="Hexo 3.9.0"><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><title>NPLM与word2vec实战 | superzhaoyang</title><meta name="description" content="NPLM与word2vec实战"><meta name="keywords" content="deep learning,NPLM,Word2Vec"><meta name="author" content="superzhaoyang"><meta name="copyright" content="superzhaoyang"><meta name="format-detection" content="telephone=no"><link rel="shortcut icon" href="/img/favicon.ico"><link rel="preconnect" href="//cdn.jsdelivr.net"><meta name="twitter:card" content="summary_large_image"><meta name="twitter:title" content="NPLM与word2vec实战"><meta name="twitter:description" content="NPLM与word2vec实战"><meta name="twitter:image" content="https://s1.ax1x.com/2020/05/11/YGUNJe.png"><meta property="og:type" content="article"><meta property="og:title" content="NPLM与word2vec实战"><meta property="og:url" content="http://yoursite.com/2020/05/11/NPLM&amp;&amp;word2vec/"><meta property="og:site_name" content="superzhaoyang"><meta property="og:description" content="NPLM与word2vec实战"><meta property="og:image" content="https://s1.ax1x.com/2020/05/11/YGUNJe.png"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><script src="https://cdn.jsdelivr.net/npm/js-cookie/dist/js.cookie.min.js"></script><script>const autoChangeMode = 'true'
var t = Cookies.get("theme");
if (autoChangeMode == '1'){
const isDarkMode = window.matchMedia("(prefers-color-scheme: dark)").matches
const isLightMode = window.matchMedia("(prefers-color-scheme: light)").matches
const isNotSpecified = window.matchMedia("(prefers-color-scheme: no-preference)").matches
const hasNoSupport = !isDarkMode && !isLightMode && !isNotSpecified

if (t === undefined){
  if (isLightMode) activateLightMode()
  else if (isDarkMode) activateDarkMode()
  else if (isNotSpecified || hasNoSupport){
    console.log('You specified no preference for a color scheme or your browser does not support it. I Schedule dark mode during night time.')
    now = new Date();
    hour = now.getHours();
    isNight = hour < 6 || hour >= 18
    isNight ? activateDarkMode() : activateLightMode()
}
} else if (t == 'light') activateLightMode()
else activateDarkMode()


} else if (autoChangeMode == '2'){
  now = new Date();
  hour = now.getHours();
  isNight = hour < 6 || hour >= 18
  if(t === undefined) isNight? activateDarkMode() : activateLightMode()
  else if (t === 'light') activateLightMode()
  else activateDarkMode() 
} else {
  if ( t == 'dark' ) activateDarkMode()
  else if ( t == 'light') activateLightMode()
}

function activateDarkMode(){
  document.documentElement.setAttribute('data-theme', 'dark')
  if (document.querySelector('meta[name="theme-color"]') !== null){
    document.querySelector('meta[name="theme-color"]').setAttribute('content','#000')
  }
}
function activateLightMode(){
  document.documentElement.setAttribute('data-theme', 'light')
  if (document.querySelector('meta[name="theme-color"]') !== null){
  document.querySelector('meta[name="theme-color"]').setAttribute('content','#fff')
  }
}</script><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="/fonts/font-awesome.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/node-snackbar@0.1.15/dist/snackbar.min.css"><link rel="canonical" href="http://yoursite.com/2020/05/11/NPLM&amp;&amp;word2vec/"><link rel="next" title="YunDisk" href="http://yoursite.com/2020/05/09/YunDisk/"><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Titillium+Web"><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"search.xml","languages":{"hits_empty":"We didn't find any results for the search: ${query}"}},
  translate: {"defaultEncoding":2,"translateDelay":0,"cookieDomain":"https://www.superzhaoyang.top/","msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"简"},
  highlight_copy: 'true',
  highlight_lang: 'true',
  highlight_shrink: 'false',
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  },
  bookmark: {
    title: 'Snackbar.bookmark.title',
    message_prev: 'Press',
    message_next: 'to bookmark this page'
  },
  runtime_unit: 'days',
  copyright: undefined,
  copy_copyright_js: false,
  ClickShowText: {"text":"superzhaoyang,帅,聪明,IT,敲代码,LOL,玩,酷,绚,NB,乒乓球,付同有我儿子","fontSize":"15px"},
  medium_zoom: 'true',
  Snackbar: {"bookmark":{"title":"Snackbar.bookmark.title","message_prev":"Press","message_next":"to bookmark this page"},"chs_to_cht":"Traditional Chinese Activated Manually","cht_to_chs":"Simplified Chinese Activated Manually","day_to_night":"Light Mode Activated Manually","night_to_day":"Dark Mode Activated Manually","bgLight":"#49b1f5","bgDark":"#2d3035","position":"bottom-left"}
  
}</script></head><body><canvas class="fireworks"></canvas><div id="header"> <div id="page-header"><span class="pull_left" id="blog_name"><a class="blog_title" id="site-name" href="/">superzhaoyang</a></span><i class="fa fa-bars fa-fw toggle-menu pull_right close" aria-hidden="true"></i><span class="pull_right menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fa fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fa fa-archive"></i><span> 档案</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fa fa-folder-open"></i><span> 类别</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fa fa-link"></i><span> 链接</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fa fa-heart"></i><span> 关于</span></a></div><div class="menus_item"><a class="site-page"><i class="fa-fw fa fa-list" aria-hidden="true"></i><span> 学习和娱乐</span><i class="fa fa-chevron-down menus-expand" aria-hidden="true"></i></a><ul class="menus_item_child"><li><a class="site-page" href="/music/"><i class="fa-fw fa fa-music"></i><span> 音乐</span></a></li><li><a class="site-page" href="/movies/"><i class="fa-fw fa fa-film"></i><span> 电影</span></a></li><li><a class="site-page" href="/book/"><i class="fa-fw fa fa-book"></i><span> 书籍</span></a></li></ul></div></div></span><span class="pull_right" id="search_button"><a class="site-page social-icon search"><i class="fa fa-search fa-fw"></i><span> Search</span></a></span></div></div><div id="mobile-sidebar"><div id="menu_mask"></div><div id="mobile-sidebar-menus"><div class="mobile_author_icon"><img class="lazyload avatar_img" src="https://s2.ax1x.com/2019/09/10/naEP1g.jpg" onerror="onerror=null;src='/img/friend_404.gif'"></div><div class="mobile_post_data"><div class="mobile_data_item is_center"><div class="mobile_data_link"><a href="/archives/"><div class="headline">Articles</div><div class="length_num">59</div></a></div></div><div class="mobile_data_item is_center">      <div class="mobile_data_link"><a href="/tags/"><div class="headline">Tags</div><div class="length_num">83</div></a></div></div></div><hr><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fa fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fa fa-archive"></i><span> 档案</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fa fa-folder-open"></i><span> 类别</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fa fa-link"></i><span> 链接</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fa fa-heart"></i><span> 关于</span></a></div><div class="menus_item"><a class="site-page"><i class="fa-fw fa fa-list" aria-hidden="true"></i><span> 学习和娱乐</span><i class="fa fa-chevron-down menus-expand" aria-hidden="true"></i></a><ul class="menus_item_child"><li><a class="site-page" href="/music/"><i class="fa-fw fa fa-music"></i><span> 音乐</span></a></li><li><a class="site-page" href="/movies/"><i class="fa-fw fa fa-film"></i><span> 电影</span></a></li><li><a class="site-page" href="/book/"><i class="fa-fw fa fa-book"></i><span> 书籍</span></a></li></ul></div></div></div><div id="mobile-sidebar-toc"><div class="toc_mobile_headline">Catalog</div><ol class="toc_mobile_items"><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#一-NPLM：神经语言概率模型"><span class="toc_mobile_items-number">1.</span> <span class="toc_mobile_items-text">一.NPLM：神经语言概率模型</span></a><ol class="toc_mobile_items-child"><li class="toc_mobile_items-item toc_mobile_items-level-4"><a class="toc_mobile_items-link" href="#1-1-NPLM的基本思想"><span class="toc_mobile_items-number">1.1.</span> <span class="toc_mobile_items-text">1.1 NPLM的基本思想</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-4"><a class="toc_mobile_items-link" href="#1-2-NPLM实战"><span class="toc_mobile_items-number">1.2.</span> <span class="toc_mobile_items-text">1.2 NPLM实战</span></a></li></ol></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#1-文本预处理"><span class="toc_mobile_items-number">2.</span> <span class="toc_mobile_items-text">1. 文本预处理</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#二-Word2Vec"><span class="toc_mobile_items-number">3.</span> <span class="toc_mobile_items-text">二 .Word2Vec</span></a><ol class="toc_mobile_items-child"><li class="toc_mobile_items-item toc_mobile_items-level-4"><a class="toc_mobile_items-link" href="#2-1-Word2Vec-介绍"><span class="toc_mobile_items-number">3.1.</span> <span class="toc_mobile_items-text">2.1 Word2Vec 介绍</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-4"><a class="toc_mobile_items-link" href="#2-2-层级软最大"><span class="toc_mobile_items-number">3.2.</span> <span class="toc_mobile_items-text">2.2 层级软最大</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-4"><a class="toc_mobile_items-link" href="#2-3-负采样"><span class="toc_mobile_items-number">3.3.</span> <span class="toc_mobile_items-text">2.3 负采样</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-4"><a class="toc_mobile_items-link" href="#2-4-Word2vec实战"><span class="toc_mobile_items-number">3.4.</span> <span class="toc_mobile_items-text">2.4 Word2vec实战</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-4"><a class="toc_mobile_items-link" href="#2-5-类比关系实验"><span class="toc_mobile_items-number">3.5.</span> <span class="toc_mobile_items-text">2.5 类比关系实验</span></a></li></ol></li></ol></div></div><div id="body-wrap"><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true">     </i><div class="auto_open" id="sidebar"><div class="sidebar-toc"><div class="sidebar-toc__title">Catalog</div><div class="sidebar-toc__progress"><span class="progress-notice">You've read</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar">     </div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-3"><a class="toc-link" href="#一-NPLM：神经语言概率模型"><span class="toc-number">1.</span> <span class="toc-text">一.NPLM：神经语言概率模型</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-1-NPLM的基本思想"><span class="toc-number">1.1.</span> <span class="toc-text">1.1 NPLM的基本思想</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#1-2-NPLM实战"><span class="toc-number">1.2.</span> <span class="toc-text">1.2 NPLM实战</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-文本预处理"><span class="toc-number">2.</span> <span class="toc-text">1. 文本预处理</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#二-Word2Vec"><span class="toc-number">3.</span> <span class="toc-text">二 .Word2Vec</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#2-1-Word2Vec-介绍"><span class="toc-number">3.1.</span> <span class="toc-text">2.1 Word2Vec 介绍</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-2-层级软最大"><span class="toc-number">3.2.</span> <span class="toc-text">2.2 层级软最大</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-3-负采样"><span class="toc-number">3.3.</span> <span class="toc-text">2.3 负采样</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-4-Word2vec实战"><span class="toc-number">3.4.</span> <span class="toc-text">2.4 Word2vec实战</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-5-类比关系实验"><span class="toc-number">3.5.</span> <span class="toc-text">2.5 类比关系实验</span></a></li></ol></li></ol></div></div></div><div id="content-outer"><div id="top-container" style="background-image: url(https://s1.ax1x.com/2020/05/11/YGUNJe.png)"><div id="post-info"><div id="post-title"><div class="posttitle">NPLM与word2vec实战</div></div><div id="post-meta"><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> Created 2020-05-11<span class="post-meta__separator">|</span><i class="fa fa-history" aria-hidden="true"></i> Updated 2020-05-11</time><div class="post-meta-wordcount"><i class="fa fa-eye post-meta__icon" aria-hidden="true">       </i><span>Post View: </span><span id="busuanzi_value_page_pv"></span></div></div></div></div><div class="layout layout_post" id="content-inner">   <article id="post"><div class="article-container" id="post-content"><h3 id="一-NPLM：神经语言概率模型"><a href="#一-NPLM：神经语言概率模型" class="headerlink" title="一.NPLM：神经语言概率模型"></a>一.NPLM：神经语言概率模型</h3><p>​    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;我们需要一种能避免几种传统编码方式缺陷的模型。我们希望，经过这个模型编码后的词向量长度固定，但不需要太长。另外，向量最好是密集的，也就是0元素要尽可能少，不要浪费存储空间。<strong>实际上，NPLM模型刚好可以满足我们的要求。</strong></p>
<p>​    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;首先来看几个可能的例子。</p>
<ul>
<li><p>“太阳”经NPLM模型编码后的词向量可能是【0.40，0.34，-0.17，0.88】;</p>
</li>
<li><p>“月亮”经NPLM模型编码后的词向量可能是【0.41，0.30，0.55，0.90】;</p>
</li>
<li><p>“猫’’ 经NPLM模型编码后的词向量可能是【0.01，-0.50，-0.95，0.20】;</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;每个词向量的维度都是一样的。从语义上来说，”太阳“和”月亮“在向量空间中的距离更近。”猫“与”太阳“和”月亮“的距离很远。这是我们想要的结果。</p>
</li>
</ul>
<h4 id="1-1-NPLM的基本思想"><a href="#1-1-NPLM的基本思想" class="headerlink" title="1.1 NPLM的基本思想"></a>1.1 NPLM的基本思想</h4><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;NPLM是一个基于神经网络的语言模型，有趣的是，这个模型的本意并不是获取词向量的而是用读到的前几个词去预测下一个词的。词向量只不过是NPLM模型的副产品。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在学习NPLM模型之前，我们需要先简单了解一个自然语言处理中非常重要的概念N-gram模型，也叫N元语言模型。Ngam模型假设一个词语的出现只和它前面的N个词语相关按照这个假设，我们可以根据前N个词语去预测当前的词语。比如在“天空的颜色大是”这句话中，我们根据“天空”“的”“颜色”“是”这4个词语，很容易预测下一个词语是“蓝色果将这个过程抽象为N-gram模型运算过程，我们可以知道此处的N被设置为4也就是说,一个4-gram模型就可以解决当前问题。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;N-gram模型在自然语言处理领域中的应用非常广泛。理论上，我们希望能够以非常大的N 来建立N-gram模型，因为有的词语和前面的联系非常远，例如，“最近小镇旁边建立的工厂违规排放废气，造成了严重的大气污染，这里的天空的颜色是”。此处，如果用4-gram模型，我们依然会在空白处填为“蓝色”，但是如果N被设置得非常大，模型就可能会考虑“污染”“废气等词语的影响，而将此处填为“灰色”。事实上，在实际应用的过程中，将N设置得越大，模型的运算就越慢，所以在一般的项目中，我们通常将N设置为2或3。将N设置为3即可在大部分自然语言处理任务中取得较好的效果。<br>在下面的例子中，我们就选用3-gram模型，将其应用到NPLM网络中，即我们假设当前词的出现与它前面的3个词有关由此可见，N-gram模型的本质就是一个映射函数，它把前N个单词映射到下一个单词f( w,w2,…,WN)=  W N+1,而NPLM想要做的，就是用一个神经网络通过机器学习的方式来学到这个映射函数f。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;非常有意思的是，NPLM在完成这个学习任务之后，我们就可以在这个神经网络中“读出”每个单词的向量编码了。换句话说，词向量不过是NPLM的一个副产品。</p>
<h4 id="1-2-NPLM实战"><a href="#1-2-NPLM实战" class="headerlink" title="1.2 NPLM实战"></a>1.2 NPLM实战</h4><p>首先，我们构建了一个简单的NGram语言模型，根据N个历史词汇预测下一个单词，从而得到每一个单词的向量表示。我们用小说《三体》为例，展示了我们的词向量嵌入效果。</p>
<p>其次，我们学习了如何使用成熟的Google开发的Word2Vec包来进行大规模语料的词向量训练，以及如何加载已经训练好的词向量，从而利用这些词向量来做一些简单的运算和测试。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 加载必要的程序包</span></span><br><span class="line"><span class="comment"># PyTorch的程序包</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"></span><br><span class="line"><span class="comment"># 数值运算和绘图的程序包</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> matplotlib</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载机器学习的软件包</span></span><br><span class="line"><span class="keyword">from</span> sklearn.decomposition <span class="keyword">import</span> PCA</span><br><span class="line"></span><br><span class="line"><span class="comment">#加载Word2Vec的软件包</span></span><br><span class="line"><span class="keyword">import</span> gensim <span class="keyword">as</span> gensim</span><br><span class="line"><span class="keyword">from</span> gensim.models <span class="keyword">import</span> Word2Vec</span><br><span class="line"><span class="keyword">from</span> gensim.models.keyedvectors <span class="keyword">import</span> KeyedVectors</span><br><span class="line"><span class="keyword">from</span> gensim.models.word2vec <span class="keyword">import</span> LineSentence</span><br><span class="line"></span><br><span class="line"><span class="comment">#加载‘结巴’中文分词软件包</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> jieba</span><br><span class="line"></span><br><span class="line"><span class="comment">#加载正则表达式处理的包</span></span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"></span><br><span class="line">%matplotlib inline</span><br></pre></td></tr></table></figure>

<h3 id="1-文本预处理"><a href="#1-文本预处理" class="headerlink" title="1. 文本预处理"></a>1. 文本预处理</h3><p>我们以刘慈欣著名的科幻小说《三体》为例，来展示利用NGram模型训练词向量的方法</p>
<p>预处理分为两个步骤：</p>
<p>1、读取文件</p>
<p>2、分词</p>
<p>3、将语料划分为N＋1元组，准备好训练用数据</p>
<p>在这里，我们并没有去除标点符号，一是为了编程简洁，而是考虑到分词会自动将标点符号当作一个单词处理，因此不需要额外考虑。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#读入原始文件</span></span><br><span class="line"></span><br><span class="line">f = open(<span class="string">"三体.txt"</span>, <span class="string">'r'</span>)</span><br><span class="line"><span class="comment"># 若想加快运行速度，使用下面的语句（选用了三体的其中一章）：</span></span><br><span class="line"><span class="comment">#f = open("3body.txt", 'r') </span></span><br><span class="line">text = str(f.read())</span><br><span class="line">f.close()</span><br><span class="line"></span><br><span class="line">text</span><br><span class="line"></span><br><span class="line"><span class="comment"># 分词</span></span><br><span class="line">temp = jieba.lcut(text)</span><br><span class="line">words = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> temp:</span><br><span class="line">    <span class="comment">#过滤掉所有的标点符号</span></span><br><span class="line">    i = re.sub(<span class="string">"[\s+\.\!\/_,$%^*(+\"\'“”《》?“]+|[+——！，。？、~@#￥%……&amp;*（）：]+"</span>, <span class="string">""</span>, i)</span><br><span class="line">    <span class="keyword">if</span> len(i) &gt; <span class="number">0</span>:</span><br><span class="line">        words.append(i)</span><br><span class="line">print(len(words))</span><br><span class="line">words</span><br><span class="line"></span><br><span class="line"> 构建三元组列表.  每一个元素为： ([ i<span class="number">-2</span>位置的词, i<span class="number">-1</span>位置的词 ], 下一个词)</span><br><span class="line"><span class="comment"># 我们选择的Ngram中的N，即窗口大小为2</span></span><br><span class="line">trigrams = [([words[i], words[i + <span class="number">1</span>]], words[i + <span class="number">2</span>]) <span class="keyword">for</span> i <span class="keyword">in</span> range(len(words) - <span class="number">2</span>)]</span><br><span class="line"><span class="comment"># 打印出前三个元素看看</span></span><br><span class="line">print(trigrams[:<span class="number">3</span>])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 得到词汇表</span></span><br><span class="line">vocab = set(words)</span><br><span class="line">print(len(vocab))</span><br><span class="line"><span class="comment"># 两个字典，一个根据单词索引其编号，一个根据编号索引单词</span></span><br><span class="line"><span class="comment">#word_to_idx中的值包含两部分，一部分为id，另一部分为单词出现的次数</span></span><br><span class="line"><span class="comment">#word_to_idx中的每一个元素形如：&#123;w:[id, count]&#125;，其中w为一个词，id为该词的编号，count为该单词在words全文中出现的次数</span></span><br><span class="line">word_to_idx = &#123;&#125; </span><br><span class="line">idx_to_word = &#123;&#125;</span><br><span class="line">ids = <span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#对全文循环，构建这两个字典</span></span><br><span class="line"><span class="keyword">for</span> w <span class="keyword">in</span> words:</span><br><span class="line">    cnt = word_to_idx.get(w, [ids, <span class="number">0</span>])</span><br><span class="line">    <span class="keyword">if</span> cnt[<span class="number">1</span>] == <span class="number">0</span>:</span><br><span class="line">        ids += <span class="number">1</span></span><br><span class="line">    cnt[<span class="number">1</span>] += <span class="number">1</span></span><br><span class="line">    word_to_idx[w] = cnt</span><br><span class="line">    idx_to_word[ids] = w</span><br></pre></td></tr></table></figure>

<p>我们构造了一个三层的网络：</p>
<p>1、输入层：embedding层，这一层的作用是：先将输入单词的编号映射为一个one hot编码的向量，形如：001000，维度为单词表大小。 然后，embedding会通过一个线性的神经网络层映射到这个词的向量表示，输出为embedding_dim</p>
<p>2、线性层，从embedding_dim维度到128维度，然后经过非线性ReLU函数</p>
<p>3、线性层：从128维度到单词表大小维度，然后log softmax函数，给出预测每个单词的概率</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">NGram</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, vocab_size, embedding_dim, context_size)</span>:</span></span><br><span class="line">        super(NGram, self).__init__()</span><br><span class="line">        self.embeddings = nn.Embedding(vocab_size, embedding_dim)  <span class="comment">#嵌入层</span></span><br><span class="line">        self.linear1 = nn.Linear(context_size * embedding_dim, <span class="number">128</span>) <span class="comment">#线性层</span></span><br><span class="line">        self.linear2 = nn.Linear(<span class="number">128</span>, vocab_size) <span class="comment">#线性层</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, inputs)</span>:</span></span><br><span class="line">        <span class="comment">#嵌入运算，嵌入运算在内部分为两步：将输入的单词编码映射为one hot向量表示，然后经过一个线性层得到单词的词向量</span></span><br><span class="line">        <span class="comment">#inputs的尺寸为：1*context_size</span></span><br><span class="line">        embeds = self.embeddings(inputs)</span><br><span class="line">        <span class="comment">#embeds的尺寸为: context_size*embedding_dim</span></span><br><span class="line">        embeds = embeds.view(<span class="number">1</span>, <span class="number">-1</span>)</span><br><span class="line">        <span class="comment">#此时embeds的尺寸为：1*embedding_dim</span></span><br><span class="line">        <span class="comment"># 线性层加ReLU</span></span><br><span class="line">        out = self.linear1(embeds)</span><br><span class="line">        out = F.relu(out)</span><br><span class="line">        <span class="comment">#此时out的尺寸为1*128</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 线性层加Softmax</span></span><br><span class="line">        out = self.linear2(out)</span><br><span class="line">        <span class="comment">#此时out的尺寸为：1*vocab_size</span></span><br><span class="line">        log_probs = F.log_softmax(out, dim = <span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> log_probs</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">extract</span><span class="params">(self, inputs)</span>:</span></span><br><span class="line">        embeds = self.embeddings(inputs)</span><br><span class="line">        <span class="keyword">return</span> embeds</span><br><span class="line">        </span><br><span class="line">losses = [] <span class="comment">#纪录每一步的损失函数</span></span><br><span class="line">criterion = nn.NLLLoss() <span class="comment">#运用负对数似然函数作为目标函数（常用于多分类问题的目标函数）</span></span><br><span class="line">model = NGram(len(vocab), <span class="number">10</span>, <span class="number">2</span>) <span class="comment">#定义NGram模型，向量嵌入维数为10维，N（窗口大小）为2</span></span><br><span class="line">optimizer = optim.SGD(model.parameters(), lr=<span class="number">0.001</span>) <span class="comment">#使用随机梯度下降算法作为优化器</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#循环100个周期</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(<span class="number">20</span>):</span><br><span class="line">    total_loss = torch.Tensor([<span class="number">0</span>])</span><br><span class="line">    <span class="keyword">for</span> context, target <span class="keyword">in</span> trigrams:</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 准备好输入模型的数据，将词汇映射为编码</span></span><br><span class="line">        context_idxs = [word_to_idx[w][<span class="number">0</span>] <span class="keyword">for</span> w <span class="keyword">in</span> context]</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 包装成PyTorch的Variable</span></span><br><span class="line">        context_var = torch.tensor(context_idxs, dtype = torch.long)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 清空梯度：注意PyTorch会在调用backward的时候自动积累梯度信息，故而每隔周期要清空梯度信息一次。</span></span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 用神经网络做计算，计算得到输出的每个单词的可能概率对数值</span></span><br><span class="line">        log_probs = model(context_var)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 计算损失函数，同样需要把目标数据转化为编码，并包装为Variable</span></span><br><span class="line">        loss = criterion(log_probs, torch.tensor([word_to_idx[target][<span class="number">0</span>]], dtype = torch.long))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 梯度反传</span></span><br><span class="line">        loss.backward()</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 对网络进行优化</span></span><br><span class="line">        optimizer.step()</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 累加损失函数值</span></span><br><span class="line">        total_loss += loss.data</span><br><span class="line">    losses.append(total_loss)</span><br><span class="line">    print(<span class="string">'第&#123;&#125;轮，损失函数为：&#123;:.2f&#125;'</span>.format(epoch, total_loss.numpy()[<span class="number">0</span>]))</span><br></pre></td></tr></table></figure>

<p>结果展示</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 从训练好的模型中提取每个单词的向量</span></span><br><span class="line">vec = model.extract(torch.tensor([v[<span class="number">0</span>] <span class="keyword">for</span> v <span class="keyword">in</span> word_to_idx.values()], dtype = torch.long))</span><br><span class="line">vec = vec.data.numpy()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 利用PCA算法进行降维</span></span><br><span class="line">X_reduced = PCA(n_components=<span class="number">2</span>).fit_transform(vec)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 绘制所有单词向量的二维空间投影</span></span><br><span class="line">fig = plt.figure(figsize = (<span class="number">30</span>, <span class="number">20</span>))</span><br><span class="line">ax = fig.gca()</span><br><span class="line">ax.set_facecolor(<span class="string">'white'</span>)</span><br><span class="line">ax.plot(X_reduced[:, <span class="number">0</span>], X_reduced[:, <span class="number">1</span>], <span class="string">'.'</span>, markersize = <span class="number">1</span>, alpha = <span class="number">0.4</span>, color = <span class="string">'black'</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 绘制几个特殊单词的向量</span></span><br><span class="line">words = [<span class="string">'智子'</span>, <span class="string">'地球'</span>, <span class="string">'三体'</span>, <span class="string">'质子'</span>, <span class="string">'科学'</span>, <span class="string">'世界'</span>, <span class="string">'文明'</span>, <span class="string">'太空'</span>, <span class="string">'加速器'</span>, <span class="string">'平面'</span>, <span class="string">'宇宙'</span>, <span class="string">'信息'</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置中文字体，否则无法在图形上显示中文</span></span><br><span class="line">zhfont1 = matplotlib.font_manager.FontProperties(fname=<span class="string">'./华文仿宋.ttf'</span>, size=<span class="number">16</span>)</span><br><span class="line"><span class="keyword">for</span> w <span class="keyword">in</span> words:</span><br><span class="line">    <span class="keyword">if</span> w <span class="keyword">in</span> word_to_idx:</span><br><span class="line">        ind = word_to_idx[w][<span class="number">0</span>]</span><br><span class="line">        xy = X_reduced[ind]</span><br><span class="line">        plt.plot(xy[<span class="number">0</span>], xy[<span class="number">1</span>], <span class="string">'.'</span>, alpha =<span class="number">1</span>, color = <span class="string">'red'</span>)</span><br><span class="line">        plt.text(xy[<span class="number">0</span>], xy[<span class="number">1</span>], w, fontproperties = zhfont1, alpha = <span class="number">1</span>, color = <span class="string">'black'</span>)</span><br></pre></td></tr></table></figure>

<p><img alt data-src="https://s1.ax1x.com/2020/05/11/YGGzct.png" class="lazyload"></p>
<p>可以看到，这些词语在语义上并没有什么关联，这说明我们的NPLM模型学出来的词向量并不好</p>
<p>1.3 NPLM的总结与局限</p>
<p>我们可以这样理解NPLM的网络原理。实际上，它接受的输入是独热编码，在运作过程中，会先尝试将这个独热编码映射为一个特定维数的词向量，然后再用词向量取预测可能出现在这个词后面的词。随着训练次数的增加和反向传播的调整，网络渐渐获取了将意义相近的词映射为相似的词向量的能力。</p>
<p>缺点:速度太慢。NPLM于2003年诞生，但正是由于这个缺陷，没有得到广泛使用。</p>
<h3 id="二-Word2Vec"><a href="#二-Word2Vec" class="headerlink" title="二 .Word2Vec"></a>二 .Word2Vec</h3><h4 id="2-1-Word2Vec-介绍"><a href="#2-1-Word2Vec-介绍" class="headerlink" title="2.1 Word2Vec 介绍"></a>2.1 Word2Vec 介绍</h4><p>​    word2vec是NPLM的升级版，它在多方面进行了改进，大幅提升了NPLM模型的运算速度和精度。</p>
<p>​    Word2Vec是一组（2个）模型，分别叫做CBOW（continuous bag of words)模型和Skip-gram模型。</p>
<ul>
<li>CBOW:用当前词的前n个词和后n个词来预测当前的词。</li>
<li>Skip-gram:和CBOW相反，用当前词预测上下文。</li>
</ul>
<h4 id="2-2-层级软最大"><a href="#2-2-层级软最大" class="headerlink" title="2.2 层级软最大"></a>2.2 层级软最大</h4><p>在NPLM模型中，输出层单元用一层神经元来对应当前词，这种结构对于当前词的查询和反馈都是比较耗时的。层级软最大（hierarchical softmax)的核心思路是对输出层单元的结构进行更改，将其由原来的“扁平结构”编码为一个哈夫曼树（Huffman tree),树中的每一个叶节点对应一个词语。</p>
<p>  如果词典中有n个词，输出层经过哈夫曼编码后，只需要经过log(n)步即可查询到该词。而且，在每次训练的反馈过程中，都会更新从根节点到当前词对应的叶节点的所有连边的权值，这也加速了网络的training.</p>
<h4 id="2-3-负采样"><a href="#2-3-负采样" class="headerlink" title="2.3 负采样"></a>2.3 负采样</h4><p>层级软最大改进了输出层结构，而负采样则改进了目标函数。</p>
<p>核心思路：在训练过程汇总，我们不知道哪个词是正确的，而且知道随机选取的词应该是错误的。在每次训练的过程中，我们可以随机选取多个负样本一同参与损失函数的计算，模型就会同时考虑正负样本的影响。</p>
<h4 id="2-4-Word2vec实战"><a href="#2-4-Word2vec实战" class="headerlink" title="2.4 Word2vec实战"></a>2.4 Word2vec实战</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 读入文件、分词，形成一句一句的语料</span></span><br><span class="line"><span class="comment"># 注意跟前面处理不一样的地方在于，我们一行一行地读入文件，从而自然利用行将文章分开成“句子”</span></span><br><span class="line">f = open(<span class="string">"三体.txt"</span>, <span class="string">'r'</span>,encoding = <span class="string">'utf-8'</span>)</span><br><span class="line">lines = []</span><br><span class="line"><span class="keyword">for</span> line <span class="keyword">in</span> f:</span><br><span class="line">    temp = jieba.lcut(line)</span><br><span class="line">    words = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> temp:</span><br><span class="line">        <span class="comment">#过滤掉所有的标点符号</span></span><br><span class="line">        i = re.sub(<span class="string">"[\s+\.\!\/_,$%^*(+\"\'””《》]+|[+——！，。？、~@#￥%……&amp;*（）：；‘]+"</span>, <span class="string">""</span>, i)</span><br><span class="line">        <span class="keyword">if</span> len(i) &gt; <span class="number">0</span>:</span><br><span class="line">            words.append(i)</span><br><span class="line">    <span class="keyword">if</span> len(words) &gt; <span class="number">0</span>:</span><br><span class="line">        lines.append(words)</span><br><span class="line"><span class="comment"># 调用Word2Vec的算法进行训练。</span></span><br><span class="line"><span class="comment"># 参数分别为：size: 嵌入后的词向量维度；window: 上下文的宽度，min_count为考虑计算的单词的最低词频阈值</span></span><br><span class="line">model = Word2Vec(lines, size = <span class="number">20</span>, window = <span class="number">2</span> , min_count = <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">model.wv.most_similar(<span class="string">'智子'</span>, topn = <span class="number">20</span>)</span><br><span class="line"><span class="comment"># 将词向量投影到二维空间</span></span><br><span class="line">rawWordVec = []</span><br><span class="line">word2ind = &#123;&#125;</span><br><span class="line"><span class="keyword">for</span> i, w <span class="keyword">in</span> enumerate(model.wv.vocab):</span><br><span class="line">    rawWordVec.append(model[w])</span><br><span class="line">    word2ind[w] = i</span><br><span class="line">rawWordVec = np.array(rawWordVec)</span><br><span class="line">X_reduced = PCA(n_components=<span class="number">2</span>).fit_transform(rawWordVec)</span><br><span class="line"><span class="comment"># 绘制星空图</span></span><br><span class="line"><span class="comment"># 绘制所有单词向量的二维空间投影</span></span><br><span class="line">fig = plt.figure(figsize = (<span class="number">15</span>, <span class="number">10</span>))</span><br><span class="line">ax = fig.gca()</span><br><span class="line">ax.set_facecolor(<span class="string">'white'</span>)</span><br><span class="line">ax.plot(X_reduced[:, <span class="number">0</span>], X_reduced[:, <span class="number">1</span>], <span class="string">'.'</span>, markersize = <span class="number">1</span>, alpha = <span class="number">0.3</span>, color = <span class="string">'black'</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 绘制几个特殊单词的向量</span></span><br><span class="line">words = [<span class="string">'智子'</span>, <span class="string">'地球'</span>, <span class="string">'三体'</span>, <span class="string">'质子'</span>, <span class="string">'科学'</span>, <span class="string">'世界'</span>, <span class="string">'文明'</span>, <span class="string">'太空'</span>, <span class="string">'加速器'</span>, <span class="string">'平面'</span>, <span class="string">'宇宙'</span>, <span class="string">'进展'</span>,<span class="string">'的'</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置中文字体，否则无法在图形上显示中文</span></span><br><span class="line">zhfont1 = matplotlib.font_manager.FontProperties(fname=<span class="string">'./华文仿宋.ttf'</span>, size=<span class="number">16</span>)</span><br><span class="line"><span class="keyword">for</span> w <span class="keyword">in</span> words:</span><br><span class="line">    <span class="keyword">if</span> w <span class="keyword">in</span> word2ind:</span><br><span class="line">        ind = word2ind[w]</span><br><span class="line">        xy = X_reduced[ind]</span><br><span class="line">        plt.plot(xy[<span class="number">0</span>], xy[<span class="number">1</span>], <span class="string">'.'</span>, alpha =<span class="number">1</span>, color = <span class="string">'green'</span>)</span><br><span class="line">        plt.text(xy[<span class="number">0</span>], xy[<span class="number">1</span>], w, fontproperties = zhfont1, alpha = <span class="number">1</span>, color = <span class="string">'blue'</span>)</span><br><span class="line"><span class="comment"># 加载词向量</span></span><br><span class="line">word_vectors = KeyedVectors.load_word2vec_format(<span class="string">'./vectors.bin'</span>, binary=<span class="literal">True</span>, unicode_errors=<span class="string">'ignore'</span>)</span><br><span class="line">len(word_vectors.vocab)</span><br><span class="line"><span class="comment"># PCA降维</span></span><br><span class="line">rawWordVec = []</span><br><span class="line">word2ind = &#123;&#125;</span><br><span class="line"><span class="keyword">for</span> i, w <span class="keyword">in</span> enumerate(word_vectors.vocab):</span><br><span class="line">    rawWordVec.append(word_vectors[w])</span><br><span class="line">    word2ind[w] = i</span><br><span class="line">rawWordVec = np.array(rawWordVec)</span><br><span class="line">X_reduced = PCA(n_components=<span class="number">2</span>).fit_transform(rawWordVec)</span><br><span class="line"><span class="comment"># 查看相似词</span></span><br><span class="line">word_vectors.most_similar(<span class="string">'python'</span>, topn = <span class="number">20</span>)</span><br><span class="line"><span class="comment"># 绘制星空图</span></span><br><span class="line"><span class="comment"># 绘制所有的词汇</span></span><br><span class="line">fig = plt.figure(figsize = (<span class="number">30</span>, <span class="number">15</span>))</span><br><span class="line">ax = fig.gca()</span><br><span class="line">ax.set_facecolor(<span class="string">'black'</span>)</span><br><span class="line">ax.plot(X_reduced[:, <span class="number">0</span>], X_reduced[:, <span class="number">1</span>], <span class="string">'.'</span>, markersize = <span class="number">1</span>, alpha = <span class="number">0.1</span>, color = <span class="string">'white'</span>)</span><br><span class="line"></span><br><span class="line">ax.set_xlim([<span class="number">-12</span>,<span class="number">12</span>])</span><br><span class="line">ax.set_ylim([<span class="number">-10</span>,<span class="number">20</span>])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 选择几个特殊词汇，不仅画它们的位置，而且把它们的临近词也画出来</span></span><br><span class="line">words = &#123;<span class="string">'徐静蕾'</span>,<span class="string">'吴亦凡'</span>,<span class="string">'物理'</span>,<span class="string">'红楼梦'</span>,<span class="string">'量子'</span>&#125;</span><br><span class="line">all_words = []</span><br><span class="line"><span class="keyword">for</span> w <span class="keyword">in</span> words:</span><br><span class="line">    lst = word_vectors.most_similar(w)</span><br><span class="line">    wds = [i[<span class="number">0</span>] <span class="keyword">for</span> i <span class="keyword">in</span> lst]</span><br><span class="line">    metrics = [i[<span class="number">1</span>] <span class="keyword">for</span> i <span class="keyword">in</span> lst]</span><br><span class="line">    wds = np.append(wds, w)</span><br><span class="line">    all_words.append(wds)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">zhfont1 = matplotlib.font_manager.FontProperties(fname=<span class="string">'./华文仿宋.ttf'</span>, size=<span class="number">16</span>)</span><br><span class="line">colors = [<span class="string">'red'</span>, <span class="string">'yellow'</span>, <span class="string">'orange'</span>, <span class="string">'green'</span>, <span class="string">'cyan'</span>, <span class="string">'cyan'</span>]</span><br><span class="line"><span class="keyword">for</span> num, wds <span class="keyword">in</span> enumerate(all_words):</span><br><span class="line">    <span class="keyword">for</span> w <span class="keyword">in</span> wds:</span><br><span class="line">        <span class="keyword">if</span> w <span class="keyword">in</span> word2ind:</span><br><span class="line">            ind = word2ind[w]</span><br><span class="line">            xy = X_reduced[ind]</span><br><span class="line">            plt.plot(xy[<span class="number">0</span>], xy[<span class="number">1</span>], <span class="string">'.'</span>, alpha =<span class="number">1</span>, color = colors[num])</span><br><span class="line">            plt.text(xy[<span class="number">0</span>], xy[<span class="number">1</span>], w, fontproperties = zhfont1, alpha = <span class="number">1</span>, color = colors[num])</span><br><span class="line">plt.savefig(<span class="string">'88.png'</span>,dpi =<span class="number">600</span>)</span><br></pre></td></tr></table></figure>

<p>星空图如下：</p>
<p><img alt data-src="https://s1.ax1x.com/2020/05/11/YGUNJe.png" class="lazyload"></p>
<h4 id="2-5-类比关系实验"><a href="#2-5-类比关系实验" class="headerlink" title="2.5 类比关系实验"></a>2.5 类比关系实验</h4><p><img alt data-src="https://s1.ax1x.com/2020/05/11/YGUtiD.png" class="lazyload"></p>
</div></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">Author: </span><span class="post-copyright-info"><a href="mailto:undefined">superzhaoyang</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">Link: </span><span class="post-copyright-info"><a href="http://yoursite.com/2020/05/11/NPLM&amp;&amp;word2vec/">http://yoursite.com/2020/05/11/NPLM&amp;&amp;word2vec/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">Copyright Notice: </span><span class="post-copyright-info">All articles in this blog are licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> unless stating additionally.</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/deep-learning/">deep learning    </a><a class="post-meta__tags" href="/tags/NPLM/">NPLM    </a><a class="post-meta__tags" href="/tags/Word2Vec/">Word2Vec    </a></div><div class="post_share"><div class="social-share" data-image="https://s1.ax1x.com/2020/05/11/YGUNJe.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js@1.0.16/dist/css/share.min.css"><script src="https://cdn.jsdelivr.net/npm/social-share.js@1.0.16/dist/js/social-share.min.js"></script></div></div><div class="post-reward"><a class="reward-button"><i class="fa fa-qrcode"></i> Donate<div class="reward-main"><ul class="reward-all"><li class="reward-item"><img class="lazyload post-qr-code__img" src="/img/wechat.jpg"><div class="post-qr-code__desc">微信</div></li><li class="reward-item"><img class="lazyload post-qr-code__img" src="/img/alipay.jpg"><div class="post-qr-code__desc">支付宝</div></li></ul></div></a></div><nav class="pagination_post" id="pagination"><div class="next-post pull-full"><a href="/2020/05/09/YunDisk/"><img class="next_cover lazyload" data-src="https://s1.ax1x.com/2020/05/09/YQRUfK.png" onerror="onerror=null;src='/img/404.jpg'"><div class="label">Next Post</div><div class="next_info"><span>YunDisk</span></div></a></div></nav><div class="relatedPosts"><div class="relatedPosts_headline"><i class="fa fa-fw fa-thumbs-up" aria-hidden="true"></i><span> Recommend</span></div><div class="relatedPosts_list"><div class="relatedPosts_item"><a href="/2020/03/22/循环神经网络/" title="循环神经网络RNN"><img class="relatedPosts_cover lazyload" data-src="https://s1.ax1x.com/2020/03/22/84fiEd.png"><div class="relatedPosts_title">循环神经网络RNN</div></a></div><div class="relatedPosts_item"><a href="/2020/03/15/卷积神经网络实战之Lenet5-Resnet/" title="卷积神经网络实战之Lenet5 & Resnet"><img class="relatedPosts_cover lazyload" data-src="https://s1.ax1x.com/2020/03/15/88wucj.png"><div class="relatedPosts_title">卷积神经网络实战之Lenet5 & Resnet</div></a></div></div><div class="clear_both"></div></div><hr><div id="post-comment"><div class="comment_headling"><i class="fa fa-comments fa-fw" aria-hidden="true"></i><span> Comment</span></div><div class="vcomment" id="vcomment"></div><script src="https://cdn.jsdelivr.net/npm/valine/dist/Valine.min.js"></script><script>var notify = false == true ? true : false;
var verify = false == true ? true : false;
var GUEST_INFO = ['nick','mail','link'];
var guest_info = 'nick,mail,link'.split(',').filter(function(item){
  return GUEST_INFO.indexOf(item) > -1
});
guest_info = guest_info.length == 0 ? GUEST_INFO :guest_info;

window.valine = new Valine({
  el:'#vcomment',
  notify:notify,
  verify:verify,
  appId:'REItCuJxdP5eRCIkGnFNRGWt-gzGzoHsz',
  appKey:'CdrBY5eDdonSzQutnKqm0fBv',
  placeholder:'Please leave your footprints',
  avatar:'monsterid',
  guest_info:guest_info,
  pageSize:'10',
  lang:'en',
  recordIP: true
});</script></div></div></div><footer id="footer" style="background-image: url(https://s1.ax1x.com/2020/05/11/YGUNJe.png)"><div id="footer-wrap"><div class="copyright">&copy;2019 - 2020 By superzhaoyang</div><div class="framework-info"><span>Driven </span><a href="http://hexo.io"><span>Hexo</span></a><span class="footer-separator">|</span><span>Theme </span><a href="https://github.com/jerryc127/hexo-theme-butterfly"><span>Butterfly</span></a></div><div class="footer_custom_text">Hi,  welcome  to  my  <a href="https://superzhaoyang.top/">blog</a>!</div></div></footer></div><section class="rightside" id="rightside"><div id="rightside-config-hide"><i class="fa fa-book" id="readmode" title="Read Mode"></i><i class="fa fa-plus" id="font_plus" title="Increase font size"></i><i class="fa fa-minus" id="font_minus" title="Decrease font size"></i><a class="translate_chn_to_cht" id="translateLink" href="javascript:translatePage();" title="Traditional Chinese and Simplified Chinese Conversion" target="_self">繁</a><i class="darkmode fa fa-moon-o" id="darkmode" title="Dark Mode"></i></div><div id="rightside-config-show"><div id="rightside_config" title="Setting"><i class="fa fa-cog" aria-hidden="true"></i></div><a id="to_comment" href="#post-comment" title="Scroll to comment"><i class="scroll_to_comment fa fa-comments">  </i></a><i class="fa fa-list-ul close" id="mobile-toc-button" title="Table of Contents" aria-hidden="true"></i><i class="fa fa-arrow-up" id="go-up" title="Back to top" aria-hidden="true"></i></div></section><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/medium-zoom/dist/medium-zoom.min.js"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
    processEscapes: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
  },
  CommonHTML: {
    linebreaks: { automatic: true, width: "90% container" }
  },
  "HTML-CSS": { 
    linebreaks: { automatic: true, width: "90% container" }
  },
  "SVG": { 
    linebreaks: { automatic: true, width: "90% container" }
  }
});
</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
  var all = MathJax.Hub.getAllJax(), i;
  for (i=0; i < all.length; i += 1) {
    all[i].SourceElement().parentNode.className += ' has-jax';
  }
});
</script><script src="https://cdn.jsdelivr.net/npm/mathjax/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script async src="/js/search/local-search.js"></script><script src="https://cdn.jsdelivr.net/npm/animejs@latest/anime.min.js"></script><script src="/js/third-party/fireworks.js"></script><script src="https://cdn.jsdelivr.net/npm/node-snackbar@0.1.15/dist/snackbar.min.js"></script><script id="ribbon" src="/js/third-party/canvas-ribbon.js" size="150" alpha="0.6" zindex="-1" mobile="true" data-click="true"></script><script id="ribbon_piao" mobile="true" src="/js/third-party/piao.js"></script><script id="canvas_nest" color="0,0,255" opacity="0.7" zindex="-1" count="99" mobile="false" src="/js/third-party/canvas-nest.js"></script><script src="/js/baidupush.js"> </script><script src="/js/third-party/activate-power-mode.js"></script><script>POWERMODE.colorful = true; // make power mode colorful
POWERMODE.shake = true; // turn off shake
document.body.addEventListener('input', POWERMODE);
</script><script src="/js/tw_cn.js"></script><script>translateInitilization()
</script><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><script src="https://cdn.jsdelivr.net/npm/instant.page@latest/instantpage.min.js" type="module"></script><script src="https://cdn.jsdelivr.net/npm/lazysizes@latest/lazysizes.min.js" async></script><script src="/js/third-party/click_heart.js"></script><script src="/js/third-party/ClickShowText.js"></script><div class="search-dialog" id="local-search"><div class="search-dialog__title" id="local-search-title">Local search</div><div id="local-input-panel"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="Search for Posts"></div></div></div><hr><div id="local-search-results"><div id="local-hits"></div><div id="local-stats"><div class="local-search-stats__hr" id="hr"><span>Powered by</span> <a href="https://github.com/wzpan/hexo-generator-search" style="color:#49B1F5;">hexo-generator-search</a></div></div></div><span class="search-close-button"><i class="fa fa-times"></i></span></div><div class="search-mask"></div></body></html>